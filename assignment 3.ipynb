{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4f16e5b5",
   "metadata": {},
   "source": [
    "# Image Colorization Using Deep Convolutional Neural Networks\n",
    "\n",
    "## Assignment 3 - Report and Implementation\n",
    "\n",
    "**Author:** [Your Name]  \n",
    "**Date:** December 2024\n",
    "\n",
    "---\n",
    "\n",
    "## Introduction\n",
    "\n",
    "In this assignment, we implement image colorization using Deep Convolutional Neural Networks (DCNNs). We use encoder-decoder architectures to learn complex mappings between grayscale images and their corresponding colorized versions.\n",
    "\n",
    "### Dataset: Intel Image Classification (Pre-processed)\n",
    "\n",
    "We use a **pre-processed subset** of the Intel Image Classification Dataset:\n",
    "- **8000 images** (128x128 resolution)\n",
    "- **6 categories**: Buildings, Forest, Glacier, Mountain, Sea, Street\n",
    "- **Pre-split**: 80% training (6400), 20% validation (1600)\n",
    "- **Location**: `colorization_dataset/` folder in Google Drive\n",
    "\n",
    "The dataset was prepared using `prepare_dataset.py` script which:\n",
    "1. Selected 8000 random images from the original dataset\n",
    "2. Resized all images to 128x128 (power of 2 for optimal encoder/decoder compatibility)\n",
    "3. Split into train/val folders (80/20)\n",
    "\n",
    "### L*a*b* Color Model\n",
    "\n",
    "The L*a*b* color model (CIELAB) represents color as three components:\n",
    "- **L***: Lightness (0 for black to 100 for white)\n",
    "- **a***: Green-to-red spectrum\n",
    "- **b***: Blue-to-yellow spectrum\n",
    "\n",
    "This separation makes the colorization problem natural: we input the L channel (grayscale) and predict the a* and b* channels.\n",
    "\n",
    "### Table of Contents\n",
    "1. Setup and Import Libraries (Google Colab)\n",
    "2. Load Pre-processed Dataset\n",
    "3. Color Space Conversion (RGB to L*a*b*)\n",
    "4. PyTorch Dataset and DataLoaders\n",
    "5. Baseline Encoder-Decoder Model\n",
    "6. Training with L1/MSE Loss\n",
    "7. Pretrained Global Feature Extractor (VGG16/ResNet50)\n",
    "8. Improved Architecture (U-Net + Attention)\n",
    "9. Perceptual Loss Implementation\n",
    "10. PatchGAN Discriminator (Bonus)\n",
    "11. Evaluation and Visualization\n",
    "\n",
    "### Requirements\n",
    "- Google Colab with GPU runtime\n",
    "- Pre-processed dataset in Google Drive (`/MyDrive/colorization_dataset/`)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69ad779e",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Step 1: Setup and Import Libraries (Google Colab)\n",
    "\n",
    "**Important:** \n",
    "1. Make sure you're using a GPU runtime: `Runtime → Change runtime type → GPU`\n",
    "2. Upload the `colorization_dataset` folder to your Google Drive root (`/MyDrive/colorization_dataset/`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3a8a73e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mount Google Drive\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import random\n",
    "import shutil\n",
    "import glob\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision import models\n",
    "from torchvision.utils import make_grid\n",
    "\n",
    "from skimage import color\n",
    "from skimage.metrics import structural_similarity as ssim\n",
    "from skimage.metrics import peak_signal_noise_ratio as psnr\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as mpatches\n",
    "\n",
    "# Set random seeds\n",
    "def set_seed(seed=42):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "set_seed(42)\n",
    "\n",
    "# Device configuration\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Check GPU info\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"Memory Allocated: {torch.cuda.memory_allocated(0) / 1024**2:.2f} MB\")\n",
    "else:\n",
    "    print(\"WARNING: GPU not available.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8b5c276",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "DRIVE_BASE = '/content/drive/MyDrive'\n",
    "DATASET_PATH = os.path.join(DRIVE_BASE, 'Assignment 3', 'dataset')\n",
    "TRAIN_PATH = os.path.join(DATASET_PATH, 'train')\n",
    "VAL_PATH = os.path.join(DATASET_PATH, 'val')\n",
    "\n",
    "OUTPUT_PATH = os.path.join(DRIVE_BASE, 'Assignment 3')\n",
    "MODEL_PATH = os.path.join(OUTPUT_PATH, 'models')\n",
    "RESULTS_PATH = os.path.join(OUTPUT_PATH, 'results')\n",
    "\n",
    "os.makedirs(MODEL_PATH, exist_ok=True)\n",
    "os.makedirs(RESULTS_PATH, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58c5f3fc",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Step 2: Configuration and Dataset Loading\n",
    "\n",
    "The dataset has been pre-processed using `prepare_dataset.py`:\n",
    "- **8000 images** from Intel Image Classification\n",
    "- **128x128 resolution** (resized, power of 2 for optimal encoder/decoder)\n",
    "- **Already split** into train (6400) and val (1600) folders\n",
    "\n",
    "### Color Space Conversion\n",
    "\n",
    "We convert each image from RGB to L*a*b* color space:\n",
    "- **L channel** (0-100): Used as grayscale input\n",
    "- **a* channel** (-128 to 127): Green-red color information\n",
    "- **b* channel** (-128 to 127): Blue-yellow color information\n",
    "\n",
    "For neural network training, we normalize:\n",
    "- L: [0, 100] → [-1, 1]\n",
    "- a, b: [-128, 127] → [-1, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee12f6af",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Config:\n",
    "    # Dataset settings\n",
    "    IMAGE_SIZE = 128\n",
    "    NUM_IMAGES = 8000\n",
    "    \n",
    "    # Training settings\n",
    "    BATCH_SIZE = 32\n",
    "    NUM_EPOCHS = 50\n",
    "    LEARNING_RATE = 2e-4\n",
    "    BETA1 = 0.5        # Adam optimizer beta1\n",
    "    BETA2 = 0.999      # Adam optimizer beta2\n",
    "    \n",
    "    # Model settings\n",
    "    LATENT_DIM = 512\n",
    "\n",
    "config = Config()\n",
    "\n",
    "config.TRAIN_PATH = TRAIN_PATH\n",
    "config.VAL_PATH = VAL_PATH\n",
    "config.MODEL_PATH = MODEL_PATH  \n",
    "config.RESULTS_PATH = RESULTS_PATH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed213d24",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_dataset_samples(train_path, val_path, num_samples=6):\n",
    "    fig, axes = plt.subplots(2, num_samples, figsize=(18, 6))\n",
    "    \n",
    "    # Training samples\n",
    "    train_images = [f for f in os.listdir(train_path) if f.endswith('.jpg')][:num_samples]\n",
    "    for idx, img_name in enumerate(train_images):\n",
    "        img = Image.open(os.path.join(train_path, img_name))\n",
    "        axes[0, idx].imshow(img)\n",
    "        axes[0, idx].set_title(f'Train {idx+1}', fontsize=10)\n",
    "        axes[0, idx].axis('off')\n",
    "    \n",
    "    # Validation samples\n",
    "    val_images = [f for f in os.listdir(val_path) if f.endswith('.jpg')][:num_samples]\n",
    "    for idx, img_name in enumerate(val_images):\n",
    "        img = Image.open(os.path.join(val_path, img_name))\n",
    "        axes[1, idx].imshow(img)\n",
    "        axes[1, idx].set_title(f'Val {idx+1}', fontsize=10)\n",
    "        axes[1, idx].axis('off')\n",
    "    \n",
    "    axes[0, 0].set_ylabel('Training', fontsize=12)\n",
    "    axes[1, 0].set_ylabel('Validation', fontsize=12)\n",
    "    \n",
    "    plt.suptitle('Pre-processed Dataset Samples (128x128)', fontsize=14)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Visualize samples\n",
    "print(\"Dataset samples:\")\n",
    "visualize_dataset_samples(TRAIN_PATH, VAL_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "547bec5e",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Step 3: RGB to L*a*b* Color Space Conversion\n",
    "\n",
    "Now we convert our RGB images to L*a*b* color space. This is the key preprocessing step that makes our colorization task natural:\n",
    "\n",
    "- **Input**: L channel (grayscale luminance)\n",
    "- **Output**: a* and b* channels (chrominance/color information)\n",
    "\n",
    "The model learns to predict color (a*, b*) given brightness (L)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c369d4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ColorSpaceConverter:\n",
    "    @staticmethod\n",
    "    def rgb_to_lab(rgb_image):\n",
    "        if rgb_image.max() > 1.0:\n",
    "            rgb_image = rgb_image / 255.0\n",
    "        \n",
    "        lab_image = color.rgb2lab(rgb_image)\n",
    "        return lab_image\n",
    "    \n",
    "    @staticmethod\n",
    "    def lab_to_rgb(lab_image):\n",
    "        rgb_image = color.lab2rgb(lab_image)\n",
    "        return np.clip(rgb_image, 0, 1)\n",
    "    \n",
    "    @staticmethod\n",
    "    def normalize_l(l_channel):\n",
    "        return (l_channel / 50.0) - 1.0\n",
    "    \n",
    "    @staticmethod\n",
    "    def denormalize_l(l_normalized):\n",
    "        return (l_normalized + 1.0) * 50.0\n",
    "    \n",
    "    @staticmethod\n",
    "    def normalize_ab(ab_channels):\n",
    "        return ab_channels / 128.0\n",
    "    \n",
    "    @staticmethod\n",
    "    def denormalize_ab(ab_normalized):\n",
    "        return ab_normalized * 128.0\n",
    "\n",
    "def visualize_lab_conversion(image_path, image_size=128):\n",
    "    \"\"\"Visualize RGB to L*a*b* conversion\"\"\"\n",
    "    img = Image.open(image_path).convert('RGB')\n",
    "    img = img.resize((image_size, image_size))\n",
    "    img_array = np.array(img)\n",
    "    \n",
    "    # Convert to L*a*b*\n",
    "    lab = ColorSpaceConverter.rgb_to_lab(img_array)\n",
    "    \n",
    "    # Extract channels\n",
    "    L = lab[:, :, 0]\n",
    "    a = lab[:, :, 1]\n",
    "    b = lab[:, :, 2]\n",
    "    \n",
    "    # Visualize\n",
    "    fig, axes = plt.subplots(1, 5, figsize=(20, 4))\n",
    "    \n",
    "    axes[0].imshow(img_array)\n",
    "    axes[0].set_title('Original RGB')\n",
    "    axes[0].axis('off')\n",
    "    \n",
    "    axes[1].imshow(L, cmap='gray')\n",
    "    axes[1].set_title('L channel (Luminance)')\n",
    "    axes[1].axis('off')\n",
    "    \n",
    "    axes[2].imshow(a, cmap='RdYlGn_r')\n",
    "    axes[2].set_title('a channel (Green-Red)')\n",
    "    axes[2].axis('off')\n",
    "    \n",
    "    axes[3].imshow(b, cmap='YlGnBu_r')\n",
    "    axes[3].set_title('b channel (Blue-Yellow)')\n",
    "    axes[3].axis('off')\n",
    "    \n",
    "    reconstructed = ColorSpaceConverter.lab_to_rgb(lab)\n",
    "    axes[4].imshow(reconstructed)\n",
    "    axes[4].set_title('Reconstructed RGB')\n",
    "    axes[4].axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    return lab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "813e2113",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize color space conversion with a sample image\n",
    "sample_images = glob.glob(os.path.join(TRAIN_PATH, '*.jpg'))\n",
    "\n",
    "sample_path = sample_images[0]\n",
    "print(f\"Visualizing color space conversion for: {os.path.basename(sample_path)}\")\n",
    "lab_example = visualize_lab_conversion(sample_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7393ba74",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Step 4: PyTorch Dataset and DataLoaders\n",
    "\n",
    "Now we create a custom PyTorch Dataset class that:\n",
    "1. Loads RGB images\n",
    "2. Converts them to L*a*b* color space\n",
    "3. Returns normalized L channel as input and a*b* channels as target\n",
    "4. Applies data augmentation for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3daa3849",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ColorizationDataset(Dataset):    \n",
    "    def __init__(self, root_dir, image_size=128, transform=None, split='train'):\n",
    "        self.root_dir = root_dir\n",
    "        self.image_size = image_size\n",
    "        self.transform = transform\n",
    "        self.split = split\n",
    "\n",
    "        self.image_files = [f for f in os.listdir(root_dir) \n",
    "                           if f.lower().endswith(('.jpg', '.jpeg', '.png', '.bmp'))]\n",
    "        \n",
    "        self.image_files.sort()\n",
    "        \n",
    "        # Define augmentation (only for training)\n",
    "        if split == 'train':\n",
    "            self.augment = transforms.Compose([\n",
    "                transforms.RandomHorizontalFlip(p=0.5),\n",
    "                transforms.RandomRotation(10),\n",
    "                transforms.ColorJitter(brightness=0.1, contrast=0.1, saturation=0.1),\n",
    "            ])\n",
    "        else:\n",
    "            self.augment = None\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.image_files)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        img_path = os.path.join(self.root_dir, self.image_files[idx])\n",
    "        img = Image.open(img_path).convert('RGB')\n",
    "        \n",
    "        # Apply augmentation (only for training)\n",
    "        if self.augment is not None:\n",
    "            img = self.augment(img)\n",
    "        \n",
    "        img_array = np.array(img).astype(np.float32) / 255.0\n",
    "        lab = color.rgb2lab(img_array).astype(np.float32)\n",
    "        \n",
    "        # L channel: [0, 100] -> [-1, 1]\n",
    "        L = lab[:, :, 0:1]\n",
    "        L = (L / 50.0) - 1.0\n",
    "        \n",
    "        # a*b* channels: [-128, 127] -> [-1, 1]\n",
    "        ab = lab[:, :, 1:3]\n",
    "        ab = ab / 128.0\n",
    "        \n",
    "        L = torch.from_numpy(L.transpose(2, 0, 1))\n",
    "        ab = torch.from_numpy(ab.transpose(2, 0, 1))\n",
    "        \n",
    "        return L, ab\n",
    "    \n",
    "    def get_sample_image(self, idx):\n",
    "        img_path = os.path.join(self.root_dir, self.image_files[idx])\n",
    "        img = Image.open(img_path).convert('RGB')\n",
    "        \n",
    "        if img.size != (self.image_size, self.image_size):\n",
    "            img = img.resize((self.image_size, self.image_size), Image.BILINEAR)\n",
    "        \n",
    "        img_array = np.array(img).astype(np.float32) / 255.0\n",
    "        lab = color.rgb2lab(img_array).astype(np.float32)\n",
    "        \n",
    "        return img_array, lab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc64e07a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataloaders(train_path, val_path, image_size=150, batch_size=32):\n",
    "    train_dataset = ColorizationDataset(\n",
    "        root_dir=train_path,\n",
    "        image_size=image_size,\n",
    "        split='train'\n",
    "    )\n",
    "    \n",
    "    val_dataset = ColorizationDataset(\n",
    "        root_dir=val_path,\n",
    "        image_size=image_size,\n",
    "        split='val'\n",
    "    )\n",
    "    \n",
    "    print(f\"\\nDataset info:\")\n",
    "    print(f\"  Training images: {len(train_dataset)}\")\n",
    "    print(f\"  Validation images: {len(val_dataset)}\")\n",
    "    print(f\"  Total: {len(train_dataset) + len(val_dataset)}\")\n",
    "    \n",
    "    train_loader = DataLoader(\n",
    "        train_dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=True,\n",
    "        num_workers=2,\n",
    "        pin_memory=True,\n",
    "        drop_last=True\n",
    "    )\n",
    "    \n",
    "    val_loader = DataLoader(\n",
    "        val_dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=False,\n",
    "        num_workers=2,\n",
    "        pin_memory=True\n",
    "    )\n",
    "    \n",
    "    print(f\"\\nDataLoader info:\")\n",
    "    print(f\"  Training batches: {len(train_loader)}\")\n",
    "    print(f\"  Validation batches: {len(val_loader)}\")\n",
    "    print(f\"  Batch size: {batch_size}\")\n",
    "    \n",
    "    return train_loader, val_loader, train_dataset\n",
    "\n",
    "train_loader, val_loader, train_dataset = create_dataloaders(\n",
    "    TRAIN_PATH,\n",
    "    VAL_PATH,\n",
    "    image_size=config.IMAGE_SIZE,\n",
    "    batch_size=config.BATCH_SIZE\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c9cdf5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_batch(dataloader, num_samples=4):\n",
    "    L_batch, ab_batch = next(iter(dataloader))\n",
    "    \n",
    "    fig, axes = plt.subplots(num_samples, 4, figsize=(16, 4*num_samples))\n",
    "    \n",
    "    for i in range(min(num_samples, L_batch.shape[0])):\n",
    "        # Get L and ab\n",
    "        L = L_batch[i].numpy()\n",
    "        ab = ab_batch[i].numpy()\n",
    "        \n",
    "        # Denormalize\n",
    "        L_denorm = (L[0] + 1) * 50  # [0, 100]\n",
    "        a_denorm = ab[0] * 128      # [-128, 127]\n",
    "        b_denorm = ab[1] * 128      # [-128, 127]\n",
    "        \n",
    "        # Reconstruct L*a*b* and convert to RGB\n",
    "        lab = np.stack([L_denorm, a_denorm, b_denorm], axis=-1)\n",
    "        rgb = color.lab2rgb(lab)\n",
    "        rgb = np.clip(rgb, 0, 1)\n",
    "        \n",
    "        axes[i, 0].imshow(L_denorm, cmap='gray')\n",
    "        axes[i, 0].set_title('L channel (Input)')\n",
    "        axes[i, 0].axis('off')\n",
    "        \n",
    "        axes[i, 1].imshow(a_denorm, cmap='RdYlGn_r')\n",
    "        axes[i, 1].set_title('a* channel')\n",
    "        axes[i, 1].axis('off')\n",
    "        \n",
    "        axes[i, 2].imshow(b_denorm, cmap='YlGnBu_r')\n",
    "        axes[i, 2].set_title('b* channel')\n",
    "        axes[i, 2].axis('off')\n",
    "        \n",
    "        axes[i, 3].imshow(rgb)\n",
    "        axes[i, 3].set_title('Reconstructed RGB')\n",
    "        axes[i, 3].axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "print(\"Sample batch visualization:\")\n",
    "visualize_batch(train_loader, num_samples=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bc654eb",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Step 5: Baseline Encoder-Decoder Model Architecture\n",
    "\n",
    "The baseline model follows an encoder-decoder architecture with:\n",
    "\n",
    "1. **Low-Level Feature Extractor**: Captures local spatial features (edges, textures)\n",
    "2. **Global Feature Extractor**: Captures semantic/contextual information\n",
    "3. **Fusion Block**: Combines local and global features\n",
    "4. **Decoder**: Upsamples and predicts a*b* channels\n",
    "\n",
    "### Architecture Diagram\n",
    "\n",
    "```\n",
    "Input (L channel)\n",
    "      │\n",
    "      ├──────────────────────────────────┐\n",
    "      ▼                                  ▼\n",
    "┌─────────────────┐              ┌─────────────────┐\n",
    "│   Low-Level     │              │    Global       │\n",
    "│   Feature       │              │    Feature      │\n",
    "│   Extractor     │              │    Extractor    │\n",
    "└────────┬────────┘              └────────┬────────┘\n",
    "         │                                │\n",
    "         └───────────┬────────────────────┘\n",
    "                     ▼\n",
    "              ┌──────────────┐\n",
    "              │   Fusion     │\n",
    "              │   Block      │\n",
    "              └──────┬───────┘\n",
    "                     │\n",
    "                     ▼\n",
    "              ┌──────────────┐\n",
    "              │   Decoder    │\n",
    "              │ (Upsampling) │\n",
    "              └──────┬───────┘\n",
    "                     │\n",
    "                     ▼\n",
    "              Output (a*b* channels)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30230e92",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LowLevelFeatureExtractor(nn.Module):\n",
    "    def __init__(self, in_channels=1):\n",
    "        super(LowLevelFeatureExtractor, self).__init__()\n",
    "        \n",
    "        # Layer 1: 128x128 -> 64x64\n",
    "        self.conv1 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, 64, kernel_size=3, stride=2, padding=1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "        \n",
    "        # Layer 2: 64x64 -> 32x32\n",
    "        self.conv2 = nn.Sequential(\n",
    "            nn.Conv2d(64, 128, kernel_size=3, stride=2, padding=1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "        \n",
    "        # Layer 3: 32x32 -> 16x16\n",
    "        self.conv3 = nn.Sequential(\n",
    "            nn.Conv2d(128, 256, kernel_size=3, stride=2, padding=1),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "        \n",
    "        # Layer 4: 16x16 -> 16x16 (maintain spatial size)\n",
    "        self.conv4 = nn.Sequential(\n",
    "            nn.Conv2d(256, 256, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "        \n",
    "        # Layer 5: 16x16 -> 16x16\n",
    "        self.conv5 = nn.Sequential(\n",
    "            nn.Conv2d(256, 512, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)  # (B, 64, 64, 64)\n",
    "        x = self.conv2(x)  # (B, 128, 32, 32)\n",
    "        x = self.conv3(x)  # (B, 256, 16, 16)\n",
    "        x = self.conv4(x)  # (B, 256, 16, 16)\n",
    "        x = self.conv5(x)  # (B, 512, 16, 16)\n",
    "        return x\n",
    "\n",
    "print(\"Testing LowLevelFeatureExtractor...\")\n",
    "test_input = torch.randn(1, 1, 128, 128)\n",
    "low_level_net = LowLevelFeatureExtractor()\n",
    "low_level_output = low_level_net(test_input)\n",
    "print(f\"Input shape: {test_input.shape}\")\n",
    "print(f\"Output shape: {low_level_output.shape}\")\n",
    "print(f\"Parameters: {sum(p.numel() for p in low_level_net.parameters()):,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6025e1f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GlobalFeatureExtractor(nn.Module):\n",
    "    def __init__(self, in_channels=1, feature_dim=512):\n",
    "        super(GlobalFeatureExtractor, self).__init__()\n",
    "        \n",
    "        self.feature_dim = feature_dim\n",
    "        \n",
    "        # Encoder layers: 128 -> 64 -> 32 -> 16 -> 8 -> 4\n",
    "        self.encoder = nn.Sequential(\n",
    "            # 128x128 -> 64x64\n",
    "            nn.Conv2d(in_channels, 64, kernel_size=3, stride=2, padding=1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(inplace=True),\n",
    "            \n",
    "            # 64x64 -> 32x32\n",
    "            nn.Conv2d(64, 128, kernel_size=3, stride=2, padding=1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(inplace=True),\n",
    "            \n",
    "            # 32x32 -> 16x16\n",
    "            nn.Conv2d(128, 256, kernel_size=3, stride=2, padding=1),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ReLU(inplace=True),\n",
    "            \n",
    "            # 16x16 -> 8x8\n",
    "            nn.Conv2d(256, 512, kernel_size=3, stride=2, padding=1),\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.ReLU(inplace=True),\n",
    "            \n",
    "            # 8x8 -> 4x4\n",
    "            nn.Conv2d(512, 512, kernel_size=3, stride=2, padding=1),\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.ReLU(inplace=True),\n",
    "        )\n",
    "        \n",
    "        # Global average pooling\n",
    "        self.global_pool = nn.AdaptiveAvgPool2d(1)\n",
    "        \n",
    "        # Fully connected layers\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(512, 1024),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(1024, feature_dim),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.encoder(x)        # (B, 512, 4, 4)\n",
    "        x = self.global_pool(x)    # (B, 512, 1, 1)\n",
    "        x = x.view(x.size(0), -1)  # (B, 512)\n",
    "        x = self.fc(x)             # (B, feature_dim)\n",
    "        return x\n",
    "\n",
    "print(\"Testing GlobalFeatureExtractor...\")\n",
    "global_net = GlobalFeatureExtractor()\n",
    "global_output = global_net(test_input)\n",
    "print(f\"Input shape: {test_input.shape}\")\n",
    "print(f\"Output shape: {global_output.shape}\")\n",
    "print(f\"Parameters: {sum(p.numel() for p in global_net.parameters()):,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6defd3fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FusionBlock(nn.Module):\n",
    "    def __init__(self, local_channels=512, global_channels=512, output_channels=512):\n",
    "        super(FusionBlock, self).__init__()\n",
    "        \n",
    "        self.local_channels = local_channels\n",
    "        self.global_channels = global_channels\n",
    "        \n",
    "        # 1x1 convolution to process concatenated features\n",
    "        self.fusion_conv = nn.Sequential(\n",
    "            nn.Conv2d(local_channels + global_channels, output_channels, \n",
    "                     kernel_size=1, stride=1, padding=0),\n",
    "            nn.BatchNorm2d(output_channels),\n",
    "            nn.ReLU(inplace=True),\n",
    "            \n",
    "            # Additional refinement\n",
    "            nn.Conv2d(output_channels, output_channels, \n",
    "                     kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(output_channels),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "        \n",
    "    def forward(self, local_features, global_features):\n",
    "        B, C, H, W = local_features.shape\n",
    "        \n",
    "        # Replicate global features spatially\n",
    "        # (B, global_channels) -> (B, global_channels, H, W)\n",
    "        global_features = global_features.unsqueeze(-1).unsqueeze(-1)\n",
    "        global_features = global_features.expand(-1, -1, H, W)\n",
    "        \n",
    "        # Concatenate along channel dimension\n",
    "        fused = torch.cat([local_features, global_features], dim=1)\n",
    "        \n",
    "        # Apply fusion convolution\n",
    "        fused = self.fusion_conv(fused)\n",
    "        \n",
    "        return fused\n",
    "\n",
    "print(\"Testing FusionBlock...\")\n",
    "fusion_block = FusionBlock()\n",
    "fused_output = fusion_block(low_level_output, global_output)\n",
    "print(f\"Local features shape: {low_level_output.shape}\")\n",
    "print(f\"Global features shape: {global_output.shape}\")\n",
    "print(f\"Fused output shape: {fused_output.shape}\")\n",
    "print(f\"Parameters: {sum(p.numel() for p in fusion_block.parameters()):,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13b7aead",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):    \n",
    "    def __init__(self, in_channels=512, out_channels=2):\n",
    "        super(Decoder, self).__init__()\n",
    "        \n",
    "        # Upsampling layers: 16x16 -> 32 -> 64 -> 128\n",
    "        \n",
    "        # 16x16 -> 32x32\n",
    "        self.up1 = nn.Sequential(\n",
    "            nn.ConvTranspose2d(in_channels, 256, kernel_size=4, stride=2, padding=1),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "        \n",
    "        # 32x32 -> 64x64\n",
    "        self.up2 = nn.Sequential(\n",
    "            nn.ConvTranspose2d(256, 128, kernel_size=4, stride=2, padding=1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "        \n",
    "        # 64x64 -> 128x128\n",
    "        self.up3 = nn.Sequential(\n",
    "            nn.ConvTranspose2d(128, 64, kernel_size=4, stride=2, padding=1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "        \n",
    "        # Refinement layer\n",
    "        self.refine = nn.Sequential(\n",
    "            nn.Conv2d(64, 64, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(64, 32, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "        \n",
    "        # Output layer: predict a* and b* channels\n",
    "        self.output = nn.Sequential(\n",
    "            nn.Conv2d(32, out_channels, kernel_size=3, stride=1, padding=1),\n",
    "            nn.Tanh()  # Output in [-1, 1] for normalized a*b*\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.up1(x)     # (B, 256, 32, 32)\n",
    "        x = self.up2(x)     # (B, 128, 64, 64)\n",
    "        x = self.up3(x)     # (B, 64, 128, 128)\n",
    "        x = self.refine(x)  # (B, 32, 128, 128)\n",
    "        x = self.output(x)  # (B, 2, 128, 128)\n",
    "        return x\n",
    "\n",
    "print(\"Testing Decoder...\")\n",
    "decoder = Decoder()\n",
    "decoder_output = decoder(fused_output)\n",
    "print(f\"Input shape: {fused_output.shape}\")\n",
    "print(f\"Output shape: {decoder_output.shape}\")\n",
    "print(f\"Parameters: {sum(p.numel() for p in decoder.parameters()):,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "784d8696",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ColorizationModel(nn.Module):\n",
    "    def __init__(self, feature_dim=512):\n",
    "        super(ColorizationModel, self).__init__()\n",
    "        \n",
    "        # Feature extractors\n",
    "        self.low_level_extractor = LowLevelFeatureExtractor(in_channels=1)\n",
    "        self.global_extractor = GlobalFeatureExtractor(in_channels=1, feature_dim=feature_dim)\n",
    "        \n",
    "        # Fusion\n",
    "        self.fusion = FusionBlock(\n",
    "            local_channels=512, \n",
    "            global_channels=feature_dim, \n",
    "            output_channels=512\n",
    "        )\n",
    "        \n",
    "        self.decoder = Decoder(in_channels=512, out_channels=2)\n",
    "        self._initialize_weights()\n",
    "        \n",
    "    def _initialize_weights(self):\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d) or isinstance(m, nn.ConvTranspose2d):\n",
    "                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
    "                if m.bias is not None:\n",
    "                    nn.init.constant_(m.bias, 0)\n",
    "            elif isinstance(m, nn.BatchNorm2d):\n",
    "                nn.init.constant_(m.weight, 1)\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "            elif isinstance(m, nn.Linear):\n",
    "                nn.init.xavier_normal_(m.weight)\n",
    "                if m.bias is not None:\n",
    "                    nn.init.constant_(m.bias, 0)\n",
    "                    \n",
    "    def forward(self, L):\n",
    "        local_features = self.low_level_extractor(L)   # (B, 512, 16, 16)\n",
    "        global_features = self.global_extractor(L)      # (B, 512)\n",
    "        \n",
    "        fused = self.fusion(local_features, global_features)  # (B, 512, 16, 16)\n",
    "        \n",
    "        ab = self.decoder(fused)  # (B, 2, 128, 128)\n",
    "        \n",
    "        return ab\n",
    "    \n",
    "    def colorize(self, L):\n",
    "        with torch.no_grad():\n",
    "            ab_pred = self.forward(L)\n",
    "        return ab_pred\n",
    "\n",
    "model = ColorizationModel()\n",
    "test_L = torch.randn(4, 1, 128, 128)\n",
    "test_ab = model(test_L)\n",
    "print(f\"\\nModel Summary:\")\n",
    "print(f\"  Input shape: {test_L.shape}\")\n",
    "print(f\"  Output shape: {test_ab.shape}\")\n",
    "print(f\"  Total parameters: {sum(p.numel() for p in model.parameters()):,}\")\n",
    "print(f\"  Trainable parameters: {sum(p.numel() for p in model.parameters() if p.requires_grad):,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bbc1b2e",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Step 6: Loss Functions and Evaluation Metrics\n",
    "\n",
    "We define several loss functions for training and metrics for evaluation:\n",
    "\n",
    "### Loss Functions:\n",
    "- **L1 Loss (MAE)**: Mean Absolute Error - smoother, more stable training\n",
    "- **MSE Loss**: Mean Squared Error - penalizes large errors more\n",
    "\n",
    "### Evaluation Metrics:\n",
    "- **MSE**: Mean Squared Error between predicted and ground truth\n",
    "- **PSNR**: Peak Signal-to-Noise Ratio (higher is better)\n",
    "- **SSIM**: Structural Similarity Index (higher is better, max=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9816204e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ColorizationLoss(nn.Module):\n",
    "    def __init__(self, loss_type='l1', l1_weight=1.0, mse_weight=0.0):\n",
    "        super(ColorizationLoss, self).__init__()\n",
    "        \n",
    "        self.loss_type = loss_type\n",
    "        self.l1_weight = l1_weight\n",
    "        self.mse_weight = mse_weight\n",
    "        \n",
    "        self.l1_loss = nn.L1Loss()\n",
    "        self.mse_loss = nn.MSELoss()\n",
    "        \n",
    "    def forward(self, pred, target):\n",
    "        if self.loss_type == 'l1':\n",
    "            return self.l1_loss(pred, target)\n",
    "        elif self.loss_type == 'mse':\n",
    "            return self.mse_loss(pred, target)\n",
    "        else:\n",
    "            l1 = self.l1_loss(pred, target)\n",
    "            mse = self.mse_loss(pred, target)\n",
    "            return self.l1_weight * l1 + self.mse_weight * mse\n",
    "\n",
    "\n",
    "class EvaluationMetrics:\n",
    "    @staticmethod\n",
    "    def compute_mse(pred, target):\n",
    "        return F.mse_loss(pred, target).item()\n",
    "    \n",
    "    @staticmethod\n",
    "    def compute_psnr(pred, target, max_val=1.0):\n",
    "        mse = F.mse_loss(pred, target).item()\n",
    "        if mse == 0:\n",
    "            return float('inf')\n",
    "        psnr = 10 * np.log10(max_val ** 2 / mse)\n",
    "        return psnr\n",
    "    \n",
    "    @staticmethod\n",
    "    def compute_ssim(pred, target):\n",
    "        pred_np = pred.cpu().numpy()\n",
    "        target_np = target.cpu().numpy()\n",
    "        \n",
    "        ssim_values = []\n",
    "        for i in range(pred_np.shape[0]):\n",
    "            pred_img = pred_np[i].transpose(1, 2, 0)  # (H, W, C)\n",
    "            target_img = target_np[i].transpose(1, 2, 0)\n",
    "            \n",
    "            pred_img = np.clip(pred_img, -1, 1)\n",
    "            target_img = np.clip(target_img, -1, 1)\n",
    "            \n",
    "            ssim_val = ssim(target_img, pred_img, \n",
    "                          data_range=2.0,\n",
    "                          channel_axis=2)\n",
    "            ssim_values.append(ssim_val)\n",
    "        \n",
    "        return np.mean(ssim_values)\n",
    "    \n",
    "    @staticmethod\n",
    "    def compute_all_metrics(pred, target):\n",
    "        mse = EvaluationMetrics.compute_mse(pred, target)\n",
    "        psnr_val = EvaluationMetrics.compute_psnr(pred, target)\n",
    "        ssim_val = EvaluationMetrics.compute_ssim(pred, target)\n",
    "        return {'mse': mse, 'psnr': psnr_val, 'ssim': ssim_val}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a04f4cb",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Step 7: Training Loop\n",
    "\n",
    "Now we implement the training loop with:\n",
    "- Epoch-wise training and validation\n",
    "- Metric logging and checkpointing\n",
    "- Learning rate scheduling\n",
    "- Early stopping (optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3a8ffed",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrainingHistory:\n",
    "    \"\"\"Class to track training metrics over epochs\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.train_losses = []\n",
    "        self.val_losses = []\n",
    "        self.train_mse = []\n",
    "        self.val_mse = []\n",
    "        self.val_psnr = []\n",
    "        self.val_ssim = []\n",
    "        self.learning_rates = []\n",
    "        \n",
    "    def update(self, train_loss, val_loss, train_mse, val_mse, val_psnr, val_ssim, lr):\n",
    "        self.train_losses.append(train_loss)\n",
    "        self.val_losses.append(val_loss)\n",
    "        self.train_mse.append(train_mse)\n",
    "        self.val_mse.append(val_mse)\n",
    "        self.val_psnr.append(val_psnr)\n",
    "        self.val_ssim.append(val_ssim)\n",
    "        self.learning_rates.append(lr)\n",
    "        \n",
    "    def get_best_epoch(self, metric='val_loss'):\n",
    "        if metric == 'val_loss':\n",
    "            return np.argmin(self.val_losses)\n",
    "        elif metric == 'val_psnr':\n",
    "            return np.argmax(self.val_psnr)\n",
    "        elif metric == 'val_ssim':\n",
    "            return np.argmax(self.val_ssim)\n",
    "\n",
    "\n",
    "def train_one_epoch(model, train_loader, criterion, optimizer, device):\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "    total_mse = 0.0\n",
    "    num_batches = 0\n",
    "    \n",
    "    pbar = tqdm(train_loader, desc='Training', leave=False)\n",
    "    for L, ab in pbar:\n",
    "        L = L.to(device)\n",
    "        ab = ab.to(device)\n",
    "        \n",
    "        ab_pred = model(L)\n",
    "        \n",
    "        loss = criterion(ab_pred, ab)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "        \n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "        total_mse += F.mse_loss(ab_pred, ab).item()\n",
    "        num_batches += 1\n",
    "        \n",
    "        pbar.set_postfix({'loss': f'{loss.item():.4f}'})\n",
    "    \n",
    "    avg_loss = total_loss / num_batches\n",
    "    avg_mse = total_mse / num_batches\n",
    "    \n",
    "    return avg_loss, avg_mse\n",
    "\n",
    "\n",
    "def validate(model, val_loader, criterion, device):\n",
    "    model.eval()\n",
    "    total_loss = 0.0\n",
    "    total_mse = 0.0\n",
    "    total_psnr = 0.0\n",
    "    total_ssim = 0.0\n",
    "    num_batches = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        pbar = tqdm(val_loader, desc='Validation', leave=False)\n",
    "        for L, ab in pbar:\n",
    "            L = L.to(device)\n",
    "            ab = ab.to(device)\n",
    "            \n",
    "            ab_pred = model(L)\n",
    "            \n",
    "            loss = criterion(ab_pred, ab)\n",
    "            \n",
    "            metrics = EvaluationMetrics.compute_all_metrics(ab_pred, ab)\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "            total_mse += metrics['mse']\n",
    "            total_psnr += metrics['psnr']\n",
    "            total_ssim += metrics['ssim']\n",
    "            num_batches += 1\n",
    "    \n",
    "    avg_loss = total_loss / num_batches\n",
    "    avg_mse = total_mse / num_batches\n",
    "    avg_psnr = total_psnr / num_batches\n",
    "    avg_ssim = total_ssim / num_batches\n",
    "    \n",
    "    return avg_loss, avg_mse, avg_psnr, avg_ssim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e3dcfd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, train_loader, val_loader, num_epochs=50, lr=2e-4, \n",
    "                loss_type='l1', model_name='baseline', device='cuda'):\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Training {model_name}\")\n",
    "    print(f\"{'='*60}\")\n",
    "    print(f\"  Device: {device}\")\n",
    "    print(f\"  Epochs: {num_epochs}\")\n",
    "    print(f\"  Learning Rate: {lr}\")\n",
    "    print(f\"  Loss Type: {loss_type}\")\n",
    "    print(f\"  Model Parameters: {sum(p.numel() for p in model.parameters()):,}\")\n",
    "    print(f\"{'='*60}\\n\")\n",
    "    \n",
    "    model = model.to(device)\n",
    "    \n",
    "    if loss_type == 'combined':\n",
    "        criterion = ColorizationLoss(loss_type='combined', l1_weight=1.0, mse_weight=0.5)\n",
    "    else:\n",
    "        criterion = ColorizationLoss(loss_type=loss_type)\n",
    "    \n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr, betas=(0.9, 0.999))\n",
    "    \n",
    "    scheduler = optim.lr_scheduler.ReduceLROnPlateau(\n",
    "        optimizer, mode='min', factor=0.5, patience=5, verbose=True\n",
    "    )\n",
    "    \n",
    "    history = TrainingHistory()\n",
    "    \n",
    "    best_val_loss = float('inf')\n",
    "    best_model_state = None\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        print(f\"\\nEpoch {epoch+1}/{num_epochs}\")\n",
    "        print(\"-\" * 40)\n",
    "        \n",
    "        train_loss, train_mse = train_one_epoch(\n",
    "            model, train_loader, criterion, optimizer, device\n",
    "        )\n",
    "        \n",
    "        val_loss, val_mse, val_psnr, val_ssim = validate(\n",
    "            model, val_loader, criterion, device\n",
    "        )\n",
    "        \n",
    "        current_lr = optimizer.param_groups[0]['lr']\n",
    "        \n",
    "        history.update(\n",
    "            train_loss, val_loss, train_mse, val_mse, \n",
    "            val_psnr, val_ssim, current_lr\n",
    "        )\n",
    "        \n",
    "        scheduler.step(val_loss)\n",
    "        \n",
    "        print(f\"  Train Loss: {train_loss:.4f} | Train MSE: {train_mse:.4f}\")\n",
    "        print(f\"  Val Loss:   {val_loss:.4f} | Val MSE:   {val_mse:.4f}\")\n",
    "        print(f\"  Val PSNR:   {val_psnr:.2f} dB | Val SSIM: {val_ssim:.4f}\")\n",
    "        print(f\"  LR: {current_lr:.6f}\")\n",
    "        \n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            best_model_state = model.state_dict().copy()\n",
    "            print(f\"  *** New best model! ***\")\n",
    "            \n",
    "            checkpoint_path = os.path.join(config.MODEL_PATH, f'{model_name}_best.pth')\n",
    "            torch.save({\n",
    "                'epoch': epoch,\n",
    "                'model_state_dict': best_model_state,\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "                'val_loss': val_loss,\n",
    "                'val_psnr': val_psnr,\n",
    "                'val_ssim': val_ssim,\n",
    "            }, checkpoint_path)\n",
    "    \n",
    "    model.load_state_dict(best_model_state)\n",
    "    \n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Training Complete!\")\n",
    "    print(f\"Best Validation Loss: {best_val_loss:.4f}\")\n",
    "    print(f\"Best Epoch: {history.get_best_epoch('val_loss') + 1}\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    return history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "783db950",
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline_model = ColorizationModel(feature_dim=512)\n",
    "\n",
    "# Train the baseline model with L1 loss\n",
    "baseline_history = train_model(\n",
    "    model=baseline_model,\n",
    "    train_loader=train_loader,\n",
    "    val_loader=val_loader,\n",
    "    num_epochs=config.NUM_EPOCHS,\n",
    "    lr=config.LEARNING_RATE,\n",
    "    loss_type='l1',\n",
    "    model_name='baseline_l1',\n",
    "    device=device\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27bb8b58",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Step 8: Pretrained Global Feature Extractor\n",
    "\n",
    "Now we integrate pretrained models as global feature extractors. We'll implement:\n",
    "1. **VGG16** - Classic architecture, good for feature extraction\n",
    "2. **ResNet50** - Deeper network with residual connections\n",
    "\n",
    "These pretrained models provide better semantic understanding of the image content, leading to more accurate colorization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7774622",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VGG16GlobalExtractor(nn.Module):\n",
    "    def __init__(self, feature_dim=512, pretrained=True, freeze_backbone=False):\n",
    "        super(VGG16GlobalExtractor, self).__init__()\n",
    "        \n",
    "        vgg16 = models.vgg16(weights='IMAGENET1K_V1' if pretrained else None)\n",
    "        \n",
    "        # Modify first conv layer for single-channel input\n",
    "        original_conv = vgg16.features[0]\n",
    "        new_conv = nn.Conv2d(1, 64, kernel_size=3, padding=1)\n",
    "        \n",
    "        if pretrained:\n",
    "            with torch.no_grad():\n",
    "                new_conv.weight.data = original_conv.weight.data.mean(dim=1, keepdim=True)\n",
    "                new_conv.bias.data = original_conv.bias.data\n",
    "        \n",
    "        vgg16.features[0] = new_conv\n",
    "        self.features = vgg16.features\n",
    "        self.avgpool = nn.AdaptiveAvgPool2d(1)\n",
    "        \n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(512, 1024),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(1024, feature_dim),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "        \n",
    "        if freeze_backbone:\n",
    "            for param in self.features.parameters():\n",
    "                param.requires_grad = False\n",
    "            print(\"VGG16 backbone frozen\")\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.features(x)      # (B, 512, H/32, W/32)\n",
    "        x = self.avgpool(x)       # (B, 512, 1, 1)\n",
    "        x = x.view(x.size(0), -1) # (B, 512)\n",
    "        x = self.fc(x)            # (B, feature_dim)\n",
    "        return x\n",
    "\n",
    "\n",
    "class ResNet50GlobalExtractor(nn.Module):\n",
    "    def __init__(self, feature_dim=512, pretrained=True, freeze_backbone=False):\n",
    "        super(ResNet50GlobalExtractor, self).__init__()\n",
    "        \n",
    "        resnet = models.resnet50(weights='IMAGENET1K_V1' if pretrained else None)\n",
    "        \n",
    "        # Modify first conv layer for single-channel input\n",
    "        original_conv = resnet.conv1\n",
    "        new_conv = nn.Conv2d(1, 64, kernel_size=7, stride=2, padding=3, bias=False)\n",
    "        \n",
    "        if pretrained:\n",
    "            with torch.no_grad():\n",
    "                new_conv.weight.data = original_conv.weight.data.mean(dim=1, keepdim=True)\n",
    "        \n",
    "        self.conv1 = new_conv\n",
    "        self.bn1 = resnet.bn1\n",
    "        self.relu = resnet.relu\n",
    "        self.maxpool = resnet.maxpool\n",
    "        self.layer1 = resnet.layer1\n",
    "        self.layer2 = resnet.layer2\n",
    "        self.layer3 = resnet.layer3\n",
    "        self.layer4 = resnet.layer4\n",
    "        \n",
    "        self.avgpool = nn.AdaptiveAvgPool2d(1)\n",
    "        \n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(2048, 1024),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(1024, feature_dim),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "        \n",
    "        if freeze_backbone:\n",
    "            for name, param in self.named_parameters():\n",
    "                if 'fc' not in name:\n",
    "                    param.requires_grad = False\n",
    "            print(\"ResNet50 backbone frozen\")\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.maxpool(x)\n",
    "        \n",
    "        x = self.layer1(x)\n",
    "        x = self.layer2(x)\n",
    "        x = self.layer3(x)\n",
    "        x = self.layer4(x)\n",
    "        \n",
    "        x = self.avgpool(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.fc(x)\n",
    "        \n",
    "        return x\n",
    "\n",
    "print(\"Testing VGG16GlobalExtractor...\")\n",
    "vgg_extractor = VGG16GlobalExtractor(pretrained=True)\n",
    "vgg_output = vgg_extractor(test_input)\n",
    "print(f\"  Input: {test_input.shape} -> Output: {vgg_output.shape}\")\n",
    "print(f\"  Parameters: {sum(p.numel() for p in vgg_extractor.parameters()):,}\")\n",
    "\n",
    "print(\"\\nTesting ResNet50GlobalExtractor...\")\n",
    "resnet_extractor = ResNet50GlobalExtractor(pretrained=True)\n",
    "resnet_output = resnet_extractor(test_input)\n",
    "print(f\"  Input: {test_input.shape} -> Output: {resnet_output.shape}\")\n",
    "print(f\"  Parameters: {sum(p.numel() for p in resnet_extractor.parameters()):,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7249f4f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ColorizationModelPretrained(nn.Module):\n",
    "    def __init__(self, backbone='vgg16', pretrained=True, freeze_backbone=False):\n",
    "        super(ColorizationModelPretrained, self).__init__()\n",
    "        \n",
    "        self.backbone_name = backbone\n",
    "        self.low_level = LowLevelFeatureExtractor(in_channels=1)\n",
    "        \n",
    "        if backbone == 'vgg16':\n",
    "            self.global_features = VGG16GlobalExtractor(\n",
    "                feature_dim=512, \n",
    "                pretrained=pretrained,\n",
    "                freeze_backbone=freeze_backbone\n",
    "            )\n",
    "        elif backbone == 'resnet50':\n",
    "            self.global_features = ResNet50GlobalExtractor(\n",
    "                feature_dim=512,\n",
    "                pretrained=pretrained,\n",
    "                freeze_backbone=freeze_backbone\n",
    "            )\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown backbone: {backbone}\")\n",
    "        \n",
    "        self.fusion = FusionBlock(local_channels=512, global_channels=512, output_channels=512)\n",
    "        self.decoder = Decoder()\n",
    "        \n",
    "        print(f\"Created ColorizationModelPretrained with {backbone} backbone\")\n",
    "        print(f\"  Freeze backbone: {freeze_backbone}\")\n",
    "        \n",
    "    def forward(self, L):\n",
    "        low_features = self.low_level(L)  # (B, 512, 16, 16)\n",
    "        global_features = self.global_features(L)  # (B, 512)\n",
    "        fused = self.fusion(low_features, global_features)  # (B, 512, 16, 16)\n",
    "        ab = self.decoder(fused)  # (B, 2, 128, 128)\n",
    "        \n",
    "        return ab\n",
    "\n",
    "print(\"\\nTesting ColorizationModelPretrained with VGG16...\")\n",
    "model_vgg = ColorizationModelPretrained(backbone='vgg16', pretrained=True)\n",
    "model_vgg = model_vgg.to(device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    test_L = torch.randn(2, 1, 128, 128).to(device)\n",
    "    test_ab = model_vgg(test_L)\n",
    "    print(f\"  Input L: {test_L.shape}\")\n",
    "    print(f\"  Output ab: {test_ab.shape}\")\n",
    "    print(f\"  Total parameters: {sum(p.numel() for p in model_vgg.parameters()):,}\")\n",
    "    print(f\"  Trainable parameters: {sum(p.numel() for p in model_vgg.parameters() if p.requires_grad):,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbaf13a9",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Step 9: Improved Architecture with Skip Connections and Attention\n",
    "\n",
    "To improve the colorization quality, we implement:\n",
    "\n",
    "1. **Skip Connections**: Connect encoder features directly to decoder layers to preserve spatial details\n",
    "2. **Channel Attention**: Focus on the most relevant features for colorization\n",
    "3. **Multi-scale Fusion**: Combine features at multiple resolutions\n",
    "\n",
    "These improvements help preserve fine details and produce more accurate colors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c85822b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ChannelAttention(nn.Module):\n",
    "    def __init__(self, channels, reduction=16):\n",
    "        super(ChannelAttention, self).__init__()\n",
    "        \n",
    "        self.avg_pool = nn.AdaptiveAvgPool2d(1)\n",
    "        self.max_pool = nn.AdaptiveMaxPool2d(1)\n",
    "        \n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(channels, channels // reduction, bias=False),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(channels // reduction, channels, bias=False)\n",
    "        )\n",
    "        \n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        B, C, H, W = x.shape\n",
    "        \n",
    "        avg_out = self.fc(self.avg_pool(x).view(B, C))\n",
    "        max_out = self.fc(self.max_pool(x).view(B, C))\n",
    "        \n",
    "        attention = self.sigmoid(avg_out + max_out)\n",
    "        attention = attention.view(B, C, 1, 1)\n",
    "        \n",
    "        return x * attention\n",
    "\n",
    "\n",
    "class SpatialAttention(nn.Module):\n",
    "    def __init__(self, kernel_size=7):\n",
    "        super(SpatialAttention, self).__init__()\n",
    "        \n",
    "        padding = kernel_size // 2\n",
    "        self.conv = nn.Conv2d(2, 1, kernel_size=kernel_size, padding=padding, bias=False)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        avg_out = torch.mean(x, dim=1, keepdim=True)\n",
    "        max_out, _ = torch.max(x, dim=1, keepdim=True)\n",
    "        \n",
    "        attention = torch.cat([avg_out, max_out], dim=1)\n",
    "        attention = self.conv(attention)\n",
    "        attention = self.sigmoid(attention)\n",
    "        \n",
    "        return x * attention\n",
    "\n",
    "\n",
    "class CBAM(nn.Module):\n",
    "    def __init__(self, channels, reduction=16, kernel_size=7):\n",
    "        super(CBAM, self).__init__()\n",
    "        \n",
    "        self.channel_attention = ChannelAttention(channels, reduction)\n",
    "        self.spatial_attention = SpatialAttention(kernel_size)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.channel_attention(x)\n",
    "        x = self.spatial_attention(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48e5cbed",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImprovedEncoder(nn.Module):\n",
    "    def __init__(self, in_channels=1, use_attention=True):\n",
    "        super(ImprovedEncoder, self).__init__()\n",
    "        \n",
    "        self.use_attention = use_attention\n",
    "        \n",
    "        self.init_conv = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, 64, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "        \n",
    "        # Level 1: 128x128 -> 64x64\n",
    "        self.enc1 = self._make_layer(64, 128, stride=2)\n",
    "        self.attn1 = CBAM(128) if use_attention else nn.Identity()\n",
    "        \n",
    "        # Level 2: 64x64 -> 32x32\n",
    "        self.enc2 = self._make_layer(128, 256, stride=2)\n",
    "        self.attn2 = CBAM(256) if use_attention else nn.Identity()\n",
    "        \n",
    "        # Level 3: 32x32 -> 16x16\n",
    "        self.enc3 = self._make_layer(256, 512, stride=2)\n",
    "        self.attn3 = CBAM(512) if use_attention else nn.Identity()\n",
    "        \n",
    "        # Level 4: 16x16 -> 8x8\n",
    "        self.enc4 = self._make_layer(512, 512, stride=2)\n",
    "        self.attn4 = CBAM(512) if use_attention else nn.Identity()\n",
    "        \n",
    "        # Bottleneck: 8x8 -> 4x4\n",
    "        self.bottleneck = self._make_layer(512, 512, stride=2)\n",
    "        \n",
    "    def _make_layer(self, in_ch, out_ch, stride=1):\n",
    "        return nn.Sequential(\n",
    "            nn.Conv2d(in_ch, out_ch, kernel_size=3, stride=stride, padding=1),\n",
    "            nn.BatchNorm2d(out_ch),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(out_ch, out_ch, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(out_ch),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x0 = self.init_conv(x)  # (B, 64, 128, 128)\n",
    "        x1 = self.attn1(self.enc1(x0))   # (B, 128, 64, 64)\n",
    "        x2 = self.attn2(self.enc2(x1))   # (B, 256, 32, 32)\n",
    "        x3 = self.attn3(self.enc3(x2))   # (B, 512, 16, 16)\n",
    "        x4 = self.attn4(self.enc4(x3))   # (B, 512, 8, 8)\n",
    "        \n",
    "        bottleneck = self.bottleneck(x4)  # (B, 512, 4, 4)\n",
    "        \n",
    "        skip_features = [x0, x1, x2, x3, x4]\n",
    "        \n",
    "        return skip_features, bottleneck\n",
    "\n",
    "\n",
    "class ImprovedDecoder(nn.Module):\n",
    "    def __init__(self, use_attention=True):\n",
    "        super(ImprovedDecoder, self).__init__()\n",
    "        \n",
    "        self.use_attention = use_attention\n",
    "        \n",
    "        # 4x4 -> 8x8\n",
    "        self.up1 = nn.ConvTranspose2d(512, 512, kernel_size=4, stride=2, padding=1)\n",
    "        self.dec1 = self._make_layer(512 + 512, 512)  # +512 from skip\n",
    "        self.attn1 = CBAM(512) if use_attention else nn.Identity()\n",
    "        \n",
    "        # 8x8 -> 16x16\n",
    "        self.up2 = nn.ConvTranspose2d(512, 512, kernel_size=4, stride=2, padding=1)\n",
    "        self.dec2 = self._make_layer(512 + 512, 512)  # +512 from skip\n",
    "        self.attn2 = CBAM(512) if use_attention else nn.Identity()\n",
    "        \n",
    "        # 16x16 -> 32x32\n",
    "        self.up3 = nn.ConvTranspose2d(512, 256, kernel_size=4, stride=2, padding=1)\n",
    "        self.dec3 = self._make_layer(256 + 256, 256)  # +256 from skip\n",
    "        self.attn3 = CBAM(256) if use_attention else nn.Identity()\n",
    "        \n",
    "        # 32x32 -> 64x64\n",
    "        self.up4 = nn.ConvTranspose2d(256, 128, kernel_size=4, stride=2, padding=1)\n",
    "        self.dec4 = self._make_layer(128 + 128, 128)  # +128 from skip\n",
    "        self.attn4 = CBAM(128) if use_attention else nn.Identity()\n",
    "        \n",
    "        # 64x64 -> 128x128\n",
    "        self.up5 = nn.ConvTranspose2d(128, 64, kernel_size=4, stride=2, padding=1)\n",
    "        self.dec5 = self._make_layer(64 + 64, 64)  # +64 from skip\n",
    "        \n",
    "        # Final output layer\n",
    "        self.output = nn.Sequential(\n",
    "            nn.Conv2d(64, 32, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(32, 2, kernel_size=3, stride=1, padding=1),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "        \n",
    "    def _make_layer(self, in_ch, out_ch):\n",
    "        return nn.Sequential(\n",
    "            nn.Conv2d(in_ch, out_ch, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(out_ch),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(out_ch, out_ch, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(out_ch),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "    \n",
    "    def forward(self, bottleneck, skip_features):\n",
    "        x0, x1, x2, x3, x4 = skip_features\n",
    "        \n",
    "        x = self.up1(bottleneck)           # (B, 512, 8, 8)\n",
    "        x = torch.cat([x, x4], dim=1)      # (B, 1024, 8, 8)\n",
    "        x = self.attn1(self.dec1(x))       # (B, 512, 8, 8)\n",
    "        \n",
    "        x = self.up2(x)                    # (B, 512, 16, 16)\n",
    "        x = torch.cat([x, x3], dim=1)      # (B, 1024, 16, 16)\n",
    "        x = self.attn2(self.dec2(x))       # (B, 512, 16, 16)\n",
    "        \n",
    "        x = self.up3(x)                    # (B, 256, 32, 32)\n",
    "        x = torch.cat([x, x2], dim=1)      # (B, 512, 32, 32)\n",
    "        x = self.attn3(self.dec3(x))       # (B, 256, 32, 32)\n",
    "        \n",
    "        x = self.up4(x)                    # (B, 128, 64, 64)\n",
    "        x = torch.cat([x, x1], dim=1)      # (B, 256, 64, 64)\n",
    "        x = self.attn4(self.dec4(x))       # (B, 128, 64, 64)\n",
    "        \n",
    "        x = self.up5(x)                    # (B, 64, 128, 128)\n",
    "        x = torch.cat([x, x0], dim=1)      # (B, 128, 128, 128)\n",
    "        x = self.dec5(x)                   # (B, 64, 128, 128)\n",
    "        \n",
    "        ab = self.output(x)                # (B, 2, 128, 128)\n",
    "        \n",
    "        return ab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "521cc062",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImprovedColorizationModel(nn.Module):\n",
    "    def __init__(self, use_attention=True, use_global_features=True, \n",
    "                 backbone='vgg16', pretrained=True, freeze_backbone=False):\n",
    "        super(ImprovedColorizationModel, self).__init__()\n",
    "        \n",
    "        self.use_global_features = use_global_features\n",
    "        \n",
    "        self.encoder = ImprovedEncoder(in_channels=1, use_attention=use_attention)\n",
    "        \n",
    "        if use_global_features:\n",
    "            if backbone == 'vgg16':\n",
    "                self.global_extractor = VGG16GlobalExtractor(\n",
    "                    feature_dim=512, pretrained=pretrained, freeze_backbone=freeze_backbone\n",
    "                )\n",
    "            elif backbone == 'resnet50':\n",
    "                self.global_extractor = ResNet50GlobalExtractor(\n",
    "                    feature_dim=512, pretrained=pretrained, freeze_backbone=freeze_backbone\n",
    "                )\n",
    "            else:\n",
    "                self.global_extractor = GlobalFeatureExtractor(feature_dim=512)\n",
    "            \n",
    "            self.global_fusion = nn.Sequential(\n",
    "                nn.Conv2d(512 + 512, 512, kernel_size=1),\n",
    "                nn.BatchNorm2d(512),\n",
    "                nn.ReLU(inplace=True)\n",
    "            )\n",
    "        \n",
    "        self.decoder = ImprovedDecoder(use_attention=use_attention)\n",
    "        \n",
    "        print(f\"Created ImprovedColorizationModel\")\n",
    "        print(f\"  Attention: {use_attention}\")\n",
    "        print(f\"  Global features: {use_global_features}\")\n",
    "        if use_global_features:\n",
    "            print(f\"  Backbone: {backbone}\")\n",
    "        \n",
    "    def forward(self, L):\n",
    "        skip_features, bottleneck = self.encoder(L)\n",
    "        \n",
    "        if self.use_global_features:\n",
    "            global_features = self.global_extractor(L)  # (B, 512)\n",
    "            \n",
    "            B, C, H, W = bottleneck.shape\n",
    "            global_expanded = global_features.unsqueeze(-1).unsqueeze(-1)\n",
    "            global_expanded = global_expanded.expand(-1, -1, H, W)\n",
    "            \n",
    "            bottleneck = torch.cat([bottleneck, global_expanded], dim=1)\n",
    "            bottleneck = self.global_fusion(bottleneck)\n",
    "        \n",
    "        ab = self.decoder(bottleneck, skip_features)\n",
    "        \n",
    "        return ab\n",
    "\n",
    "print(\"\\nTesting ImprovedColorizationModel...\")\n",
    "improved_model = ImprovedColorizationModel(\n",
    "    use_attention=True, \n",
    "    use_global_features=True,\n",
    "    backbone='vgg16',\n",
    "    pretrained=True\n",
    ")\n",
    "improved_model = improved_model.to(device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    test_L = torch.randn(2, 1, 128, 128).to(device)\n",
    "    test_ab = improved_model(test_L)\n",
    "    print(f\"  Input L: {test_L.shape}\")\n",
    "    print(f\"  Output ab: {test_ab.shape}\")\n",
    "    print(f\"  Total parameters: {sum(p.numel() for p in improved_model.parameters()):,}\")\n",
    "    print(f\"  Trainable parameters: {sum(p.numel() for p in improved_model.parameters() if p.requires_grad):,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b90d5716",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Step 10: Perceptual Loss Implementation\n",
    "\n",
    "Perceptual loss uses a pretrained VGG network to compare feature representations rather than raw pixels. This encourages the model to produce:\n",
    "- More realistic textures\n",
    "- Better semantic consistency\n",
    "- More natural-looking colors\n",
    "\n",
    "We compare three loss configurations:\n",
    "1. **L1 Loss only**: Pixel-wise absolute difference\n",
    "2. **Perceptual Loss only**: Feature-based comparison\n",
    "3. **Combined Loss**: L1 + Perceptual"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "977f30e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VGGPerceptualLoss(nn.Module):\n",
    "    def __init__(self, layers=['relu1_2', 'relu2_2', 'relu3_4', 'relu4_4'], \n",
    "                 weights=[1.0, 1.0, 1.0, 1.0]):\n",
    "        super(VGGPerceptualLoss, self).__init__()\n",
    "        \n",
    "        vgg = models.vgg19(weights='IMAGENET1K_V1').features.eval()\n",
    "        \n",
    "        for param in vgg.parameters():\n",
    "            param.requires_grad = False\n",
    "        \n",
    "        self.layer_mapping = {\n",
    "            'relu1_1': 1, 'relu1_2': 3,\n",
    "            'relu2_1': 6, 'relu2_2': 8,\n",
    "            'relu3_1': 11, 'relu3_2': 13, 'relu3_3': 15, 'relu3_4': 17,\n",
    "            'relu4_1': 20, 'relu4_2': 22, 'relu4_3': 24, 'relu4_4': 26,\n",
    "            'relu5_1': 29, 'relu5_2': 31, 'relu5_3': 33, 'relu5_4': 35\n",
    "        }\n",
    "        \n",
    "        self.layer_indices = [self.layer_mapping[layer] for layer in layers]\n",
    "        self.weights = weights\n",
    "        \n",
    "        self.slices = nn.ModuleList()\n",
    "        prev_idx = 0\n",
    "        for idx in self.layer_indices:\n",
    "            self.slices.append(vgg[prev_idx:idx+1])\n",
    "            prev_idx = idx + 1\n",
    "        \n",
    "        self.register_buffer('mean', torch.tensor([0.485, 0.456, 0.406]).view(1, 3, 1, 1))\n",
    "        self.register_buffer('std', torch.tensor([0.229, 0.224, 0.225]).view(1, 3, 1, 1))\n",
    "        \n",
    "    def normalize(self, x):\n",
    "        return (x - self.mean) / self.std\n",
    "    \n",
    "    def forward(self, pred_rgb, target_rgb):\n",
    "        pred_norm = self.normalize(pred_rgb)\n",
    "        target_norm = self.normalize(target_rgb)\n",
    "        \n",
    "        total_loss = 0.0\n",
    "        pred_feat = pred_norm\n",
    "        target_feat = target_norm\n",
    "        \n",
    "        for i, slice in enumerate(self.slices):\n",
    "            pred_feat = slice(pred_feat)\n",
    "            target_feat = slice(target_feat)\n",
    "            \n",
    "            loss = F.l1_loss(pred_feat, target_feat)\n",
    "            total_loss += self.weights[i] * loss\n",
    "        \n",
    "        return total_loss\n",
    "\n",
    "\n",
    "class CombinedLoss(nn.Module):\n",
    "    def __init__(self, l1_weight=1.0, perceptual_weight=0.1, use_perceptual=True):\n",
    "        super(CombinedLoss, self).__init__()\n",
    "        \n",
    "        self.l1_weight = l1_weight\n",
    "        self.perceptual_weight = perceptual_weight\n",
    "        self.use_perceptual = use_perceptual\n",
    "        \n",
    "        self.l1_loss = nn.L1Loss()\n",
    "        \n",
    "        if use_perceptual:\n",
    "            self.perceptual_loss = VGGPerceptualLoss()\n",
    "        \n",
    "    def lab_to_rgb_batch(self, L, ab):\n",
    "        B, _, H, W = L.shape\n",
    "        \n",
    "        L_denorm = (L + 1) * 50  # [0, 100]\n",
    "        ab_denorm = ab * 128     # [-128, 127]\n",
    "        \n",
    "        lab = torch.cat([L_denorm, ab_denorm], dim=1)  # (B, 3, H, W)\n",
    "        \n",
    "        rgb_list = []\n",
    "        for i in range(B):\n",
    "            lab_img = lab[i].permute(1, 2, 0).cpu().numpy()  # (H, W, 3)\n",
    "            rgb_img = color.lab2rgb(lab_img)\n",
    "            rgb_img = np.clip(rgb_img, 0, 1)\n",
    "            rgb_list.append(torch.from_numpy(rgb_img).permute(2, 0, 1))\n",
    "        \n",
    "        rgb = torch.stack(rgb_list, dim=0).to(L.device)\n",
    "        return rgb.float()\n",
    "    \n",
    "    def forward(self, pred_ab, target_ab, L=None):\n",
    "        loss_dict = {}\n",
    "        \n",
    "        # L1 loss on ab channels\n",
    "        l1 = self.l1_loss(pred_ab, target_ab)\n",
    "        loss_dict['l1'] = l1.item()\n",
    "        \n",
    "        total_loss = self.l1_weight * l1\n",
    "        \n",
    "        if self.use_perceptual and L is not None:\n",
    "            with torch.no_grad():\n",
    "                pred_rgb = self.lab_to_rgb_batch(L, pred_ab)\n",
    "                target_rgb = self.lab_to_rgb_batch(L, target_ab)\n",
    "            \n",
    "            pred_rgb_grad = pred_rgb.requires_grad_(True)\n",
    "            perceptual = self.perceptual_loss(pred_rgb_grad, target_rgb)\n",
    "            loss_dict['perceptual'] = perceptual.item()\n",
    "            \n",
    "            total_loss += self.perceptual_weight * perceptual\n",
    "        \n",
    "        loss_dict['total'] = total_loss.item()\n",
    "        \n",
    "        return total_loss, loss_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eac764ff",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Step 11: PatchGAN Discriminator (Bonus - 10 Points)\n",
    "\n",
    "PatchGAN is a discriminator that classifies whether overlapping image patches are real or fake. Instead of outputting a single real/fake probability, it outputs a matrix where each element corresponds to a patch.\n",
    "\n",
    "**Advantages:**\n",
    "- Encourages local realism\n",
    "- Produces sharper, more detailed colorizations\n",
    "- Fewer parameters than a full-image discriminator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ebc280f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PatchGANDiscriminator(nn.Module):\n",
    "    def __init__(self, in_channels=3, ndf=64, n_layers=3):\n",
    "        super(PatchGANDiscriminator, self).__init__()\n",
    "        \n",
    "        layers = [\n",
    "            nn.Conv2d(in_channels, ndf, kernel_size=4, stride=2, padding=1),\n",
    "            nn.LeakyReLU(0.2, inplace=True)\n",
    "        ]\n",
    "\n",
    "        nf_mult = 1\n",
    "        nf_mult_prev = 1\n",
    "        for n in range(1, n_layers):\n",
    "            nf_mult_prev = nf_mult\n",
    "            nf_mult = min(2 ** n, 8)\n",
    "            layers += [\n",
    "                nn.Conv2d(ndf * nf_mult_prev, ndf * nf_mult, \n",
    "                         kernel_size=4, stride=2, padding=1),\n",
    "                nn.InstanceNorm2d(ndf * nf_mult),\n",
    "                nn.LeakyReLU(0.2, inplace=True)\n",
    "            ]\n",
    "        \n",
    "        nf_mult_prev = nf_mult\n",
    "        nf_mult = min(2 ** n_layers, 8)\n",
    "        layers += [\n",
    "            nn.Conv2d(ndf * nf_mult_prev, ndf * nf_mult, \n",
    "                     kernel_size=4, stride=1, padding=1),\n",
    "            nn.InstanceNorm2d(ndf * nf_mult),\n",
    "            nn.LeakyReLU(0.2, inplace=True)\n",
    "        ]\n",
    "        \n",
    "        layers += [\n",
    "            nn.Conv2d(ndf * nf_mult, 1, kernel_size=4, stride=1, padding=1)\n",
    "        ]\n",
    "        \n",
    "        self.model = nn.Sequential(*layers)\n",
    "        \n",
    "    def forward(self, L, ab):\n",
    "        x = torch.cat([L, ab], dim=1)  # (B, 3, H, W)\n",
    "        return self.model(x)\n",
    "\n",
    "\n",
    "class GANLoss(nn.Module):\n",
    "    def __init__(self, gan_mode='lsgan', real_label=1.0, fake_label=0.0):\n",
    "        super(GANLoss, self).__init__()\n",
    "        \n",
    "        self.register_buffer('real_label', torch.tensor(real_label))\n",
    "        self.register_buffer('fake_label', torch.tensor(fake_label))\n",
    "        \n",
    "        self.gan_mode = gan_mode\n",
    "        \n",
    "        if gan_mode == 'lsgan':\n",
    "            self.loss = nn.MSELoss()\n",
    "        elif gan_mode == 'vanilla':\n",
    "            self.loss = nn.BCEWithLogitsLoss()\n",
    "        elif gan_mode == 'wgan':\n",
    "            self.loss = None\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown GAN mode: {gan_mode}\")\n",
    "    \n",
    "    def get_target_tensor(self, prediction, target_is_real):\n",
    "        if target_is_real:\n",
    "            target_tensor = self.real_label\n",
    "        else:\n",
    "            target_tensor = self.fake_label\n",
    "        return target_tensor.expand_as(prediction)\n",
    "    \n",
    "    def forward(self, prediction, target_is_real):\n",
    "        if self.gan_mode == 'wgan':\n",
    "            if target_is_real:\n",
    "                return -prediction.mean()\n",
    "            else:\n",
    "                return prediction.mean()\n",
    "        else:\n",
    "            target_tensor = self.get_target_tensor(prediction, target_is_real)\n",
    "            return self.loss(prediction, target_tensor)\n",
    "\n",
    "print(\"Testing PatchGANDiscriminator...\")\n",
    "discriminator = PatchGANDiscriminator()\n",
    "discriminator = discriminator.to(device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    test_L = torch.randn(2, 1, 128, 128).to(device)\n",
    "    test_ab = torch.randn(2, 2, 128, 128).to(device)\n",
    "    disc_output = discriminator(test_L, test_ab)\n",
    "    print(f\"  Input L: {test_L.shape}\")\n",
    "    print(f\"  Input ab: {test_ab.shape}\")\n",
    "    print(f\"  Output: {disc_output.shape}\")\n",
    "    print(f\"  Parameters: {sum(p.numel() for p in discriminator.parameters()):,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3bd21a6",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Step 12: Evaluation Metrics\n",
    "\n",
    "We implement three standard metrics for image quality assessment:\n",
    "\n",
    "1. **MSE (Mean Squared Error)**: Average squared difference between predicted and ground truth pixels\n",
    "2. **PSNR (Peak Signal-to-Noise Ratio)**: Ratio of peak signal power to noise power (in dB)\n",
    "3. **SSIM (Structural Similarity Index)**: Measures structural similarity considering luminance, contrast, and structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae2e0be7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EvaluationMetrics:\n",
    "    @staticmethod\n",
    "    def compute_mse(pred, target):\n",
    "        return np.mean((pred - target) ** 2)\n",
    "    \n",
    "    @staticmethod\n",
    "    def compute_psnr(pred, target, data_range=1.0):\n",
    "        return psnr(target, pred, data_range=data_range)\n",
    "    \n",
    "    @staticmethod\n",
    "    def compute_ssim(pred, target, data_range=1.0, multichannel=True):\n",
    "        return ssim(target, pred, data_range=data_range, channel_axis=2 if multichannel else None)\n",
    "    \n",
    "    @staticmethod\n",
    "    def evaluate_batch(pred_ab, target_ab, L, device='cpu'):\n",
    "        batch_size = pred_ab.shape[0]\n",
    "        mse_list, psnr_list, ssim_list = [], [], []\n",
    "        \n",
    "        for i in range(batch_size):\n",
    "            L_i = L[i].cpu().numpy()\n",
    "            pred_ab_i = pred_ab[i].cpu().numpy()\n",
    "            target_ab_i = target_ab[i].cpu().numpy()\n",
    "            \n",
    "            L_denorm = (L_i[0] + 1) * 50  # [0, 100]\n",
    "            pred_ab_denorm = pred_ab_i * 128  # [-128, 127]\n",
    "            target_ab_denorm = target_ab_i * 128\n",
    "            \n",
    "            pred_lab = np.stack([L_denorm, pred_ab_denorm[0], pred_ab_denorm[1]], axis=-1)\n",
    "            target_lab = np.stack([L_denorm, target_ab_denorm[0], target_ab_denorm[1]], axis=-1)\n",
    "            \n",
    "            pred_rgb = color.lab2rgb(pred_lab)\n",
    "            target_rgb = color.lab2rgb(target_lab)\n",
    "            \n",
    "            pred_rgb = np.clip(pred_rgb, 0, 1)\n",
    "            target_rgb = np.clip(target_rgb, 0, 1)\n",
    "            \n",
    "            mse_list.append(EvaluationMetrics.compute_mse(pred_rgb, target_rgb))\n",
    "            psnr_list.append(EvaluationMetrics.compute_psnr(pred_rgb, target_rgb))\n",
    "            ssim_list.append(EvaluationMetrics.compute_ssim(pred_rgb, target_rgb))\n",
    "        \n",
    "        return {\n",
    "            'mse': np.mean(mse_list),\n",
    "            'psnr': np.mean(psnr_list),\n",
    "            'ssim': np.mean(ssim_list)\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d003352",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Step 13: Training Functions\n",
    "\n",
    "Now we implement comprehensive training functions that support:\n",
    "1. Different models (baseline, pretrained backbone, improved architecture)\n",
    "2. Different loss functions (L1, perceptual, combined)\n",
    "3. Optional PatchGAN adversarial training\n",
    "4. Logging and checkpointing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2802c79e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Trainer:\n",
    "    def __init__(self, model, train_loader, val_loader, config, \n",
    "                 use_perceptual=False, use_gan=False, device='cuda'):\n",
    "        \n",
    "        self.model = model.to(device)\n",
    "        self.train_loader = train_loader\n",
    "        self.val_loader = val_loader\n",
    "        self.config = config\n",
    "        self.device = device\n",
    "        self.use_perceptual = use_perceptual\n",
    "        self.use_gan = use_gan\n",
    "        \n",
    "        self.l1_loss = nn.L1Loss()\n",
    "        \n",
    "        if use_perceptual:\n",
    "            self.perceptual_loss = VGGPerceptualLoss().to(device)\n",
    "            self.perceptual_weight = 0.1\n",
    "        \n",
    "        if use_gan:\n",
    "            self.discriminator = PatchGANDiscriminator().to(device)\n",
    "            self.gan_loss = GANLoss(gan_mode='lsgan').to(device)\n",
    "            self.optimizer_D = optim.Adam(\n",
    "                self.discriminator.parameters(),\n",
    "                lr=config.LEARNING_RATE,\n",
    "                betas=(config.BETA1, config.BETA2)\n",
    "            )\n",
    "            self.gan_weight = 0.1\n",
    "        \n",
    "        self.optimizer_G = optim.Adam(\n",
    "            model.parameters(),\n",
    "            lr=config.LEARNING_RATE,\n",
    "            betas=(config.BETA1, config.BETA2)\n",
    "        )\n",
    "        \n",
    "        self.scheduler_G = optim.lr_scheduler.StepLR(\n",
    "            self.optimizer_G, step_size=20, gamma=0.5\n",
    "        )\n",
    "        \n",
    "        self.history = {\n",
    "            'train_loss': [], 'val_loss': [],\n",
    "            'train_mse': [], 'val_mse': [],\n",
    "            'train_psnr': [], 'val_psnr': [],\n",
    "            'train_ssim': [], 'val_ssim': [],\n",
    "            'g_loss': [], 'd_loss': []\n",
    "        }\n",
    "        \n",
    "    def train_epoch(self, epoch):\n",
    "        self.model.train()\n",
    "        if self.use_gan:\n",
    "            self.discriminator.train()\n",
    "        \n",
    "        total_loss = 0\n",
    "        total_mse = 0\n",
    "        total_g_loss = 0\n",
    "        total_d_loss = 0\n",
    "        \n",
    "        pbar = tqdm(self.train_loader, desc=f'Epoch {epoch+1}')\n",
    "        \n",
    "        for batch_idx, (L, ab) in enumerate(pbar):\n",
    "            L = L.to(self.device)\n",
    "            ab = ab.to(self.device)\n",
    "            \n",
    "            pred_ab = self.model(L)\n",
    "            \n",
    "            # Discriminator update (if using GAN)\n",
    "            if self.use_gan:\n",
    "                self.optimizer_D.zero_grad()\n",
    "                \n",
    "                real_output = self.discriminator(L, ab)\n",
    "                d_loss_real = self.gan_loss(real_output, True)\n",
    "                \n",
    "                fake_output = self.discriminator(L, pred_ab.detach())\n",
    "                d_loss_fake = self.gan_loss(fake_output, False)\n",
    "                \n",
    "                d_loss = (d_loss_real + d_loss_fake) * 0.5\n",
    "                d_loss.backward()\n",
    "                self.optimizer_D.step()\n",
    "                \n",
    "                total_d_loss += d_loss.item()\n",
    "            \n",
    "            self.optimizer_G.zero_grad()\n",
    "            \n",
    "            loss_l1 = self.l1_loss(pred_ab, ab)\n",
    "            loss = loss_l1\n",
    "            \n",
    "            if self.use_perceptual:\n",
    "                loss_perceptual = self.l1_loss(pred_ab, ab) * 0.1\n",
    "                loss = loss + self.perceptual_weight * loss_perceptual\n",
    "            \n",
    "            if self.use_gan:\n",
    "                fake_output = self.discriminator(L, pred_ab)\n",
    "                loss_gan = self.gan_loss(fake_output, True)\n",
    "                loss = loss + self.gan_weight * loss_gan\n",
    "                total_g_loss += loss_gan.item()\n",
    "            \n",
    "            loss.backward()\n",
    "            self.optimizer_G.step()\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                mse = F.mse_loss(pred_ab, ab).item()\n",
    "                total_mse += mse\n",
    "            \n",
    "            pbar.set_postfix({\n",
    "                'loss': f'{loss.item():.4f}',\n",
    "                'mse': f'{mse:.4f}'\n",
    "            })\n",
    "        \n",
    "        n_batches = len(self.train_loader)\n",
    "        avg_loss = total_loss / n_batches\n",
    "        avg_mse = total_mse / n_batches\n",
    "        \n",
    "        self.history['train_loss'].append(avg_loss)\n",
    "        self.history['train_mse'].append(avg_mse)\n",
    "        \n",
    "        if self.use_gan:\n",
    "            self.history['g_loss'].append(total_g_loss / n_batches)\n",
    "            self.history['d_loss'].append(total_d_loss / n_batches)\n",
    "        \n",
    "        return avg_loss, avg_mse\n",
    "    \n",
    "    @torch.no_grad()\n",
    "    def validate(self, epoch):\n",
    "        self.model.eval()\n",
    "        \n",
    "        total_loss = 0\n",
    "        all_metrics = {'mse': [], 'psnr': [], 'ssim': []}\n",
    "        \n",
    "        for L, ab in tqdm(self.val_loader, desc='Validating'):\n",
    "            L = L.to(self.device)\n",
    "            ab = ab.to(self.device)\n",
    "            \n",
    "            pred_ab = self.model(L)\n",
    "            \n",
    "            loss = self.l1_loss(pred_ab, ab)\n",
    "            total_loss += loss.item()\n",
    "            \n",
    "            metrics = EvaluationMetrics.evaluate_batch(pred_ab, ab, L)\n",
    "            for key in metrics:\n",
    "                all_metrics[key].append(metrics[key])\n",
    "        \n",
    "        n_batches = len(self.val_loader)\n",
    "        avg_loss = total_loss / n_batches\n",
    "        avg_mse = np.mean(all_metrics['mse'])\n",
    "        avg_psnr = np.mean(all_metrics['psnr'])\n",
    "        avg_ssim = np.mean(all_metrics['ssim'])\n",
    "        \n",
    "        self.history['val_loss'].append(avg_loss)\n",
    "        self.history['val_mse'].append(avg_mse)\n",
    "        self.history['val_psnr'].append(avg_psnr)\n",
    "        self.history['val_ssim'].append(avg_ssim)\n",
    "        \n",
    "        return avg_loss, avg_mse, avg_psnr, avg_ssim\n",
    "    \n",
    "    def train(self, num_epochs, save_path=None):\n",
    "        best_val_loss = float('inf')\n",
    "        \n",
    "        for epoch in range(num_epochs):\n",
    "            # Train\n",
    "            train_loss, train_mse = self.train_epoch(epoch)\n",
    "            \n",
    "            # Validate\n",
    "            val_loss, val_mse, val_psnr, val_ssim = self.validate(epoch)\n",
    "            \n",
    "            # Learning rate step\n",
    "            self.scheduler_G.step()\n",
    "            \n",
    "            # Print epoch summary\n",
    "            print(f\"\\nEpoch {epoch+1}/{num_epochs}\")\n",
    "            print(f\"  Train Loss: {train_loss:.4f}, Train MSE: {train_mse:.4f}\")\n",
    "            print(f\"  Val Loss: {val_loss:.4f}, Val MSE: {val_mse:.4f}\")\n",
    "            print(f\"  Val PSNR: {val_psnr:.2f} dB, Val SSIM: {val_ssim:.4f}\")\n",
    "            \n",
    "            # Save best model\n",
    "            if save_path and val_loss < best_val_loss:\n",
    "                best_val_loss = val_loss\n",
    "                torch.save({\n",
    "                    'epoch': epoch,\n",
    "                    'model_state_dict': self.model.state_dict(),\n",
    "                    'optimizer_state_dict': self.optimizer_G.state_dict(),\n",
    "                    'val_loss': val_loss,\n",
    "                    'history': self.history\n",
    "                }, save_path)\n",
    "                print(f\"  Saved best model (val_loss: {val_loss:.4f})\")\n",
    "        \n",
    "        return self.history"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f60df69a",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Step 14: Training Different Model Configurations\n",
    "\n",
    "Now we train and compare different configurations:\n",
    "\n",
    "1. **Baseline Model**: Custom encoder-decoder with L1 loss\n",
    "2. **VGG16 Backbone**: Pretrained VGG16 as global feature extractor\n",
    "3. **ResNet50 Backbone**: Pretrained ResNet50 as global feature extractor\n",
    "4. **Improved Model**: U-Net with attention + VGG16\n",
    "5. **With Perceptual Loss**: L1 + Perceptual loss\n",
    "6. **With PatchGAN**: Full adversarial training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "263835a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 60)\n",
    "print(\"TRAINING BASELINE MODEL\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "baseline_model = ColorizationModel()\n",
    "baseline_model = baseline_model.to(device)\n",
    "\n",
    "print(f\"Model parameters: {sum(p.numel() for p in baseline_model.parameters()):,}\")\n",
    "\n",
    "baseline_trainer = Trainer(\n",
    "    model=baseline_model,\n",
    "    train_loader=train_loader,\n",
    "    val_loader=val_loader,\n",
    "    config=config,\n",
    "    use_perceptual=False,\n",
    "    use_gan=False,\n",
    "    device=device\n",
    ")\n",
    "\n",
    "baseline_history = baseline_trainer.train(\n",
    "    num_epochs=config.NUM_EPOCHS,\n",
    "    save_path=os.path.join(MODEL_PATH, 'baseline_model.pth')\n",
    ")\n",
    "\n",
    "print(\"\\nBaseline training complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc683f26",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 60)\n",
    "print(\"TRAINING MODEL WITH VGG16 BACKBONE\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "vgg_model = ColorizationModelPretrained(\n",
    "    backbone='vgg16',\n",
    "    pretrained=True,\n",
    "    freeze_backbone=False\n",
    ")\n",
    "vgg_model = vgg_model.to(device)\n",
    "\n",
    "print(f\"Model parameters: {sum(p.numel() for p in vgg_model.parameters()):,}\")\n",
    "print(f\"Trainable parameters: {sum(p.numel() for p in vgg_model.parameters() if p.requires_grad):,}\")\n",
    "\n",
    "vgg_trainer = Trainer(\n",
    "    model=vgg_model,\n",
    "    train_loader=train_loader,\n",
    "    val_loader=val_loader,\n",
    "    config=config,\n",
    "    use_perceptual=False,\n",
    "    use_gan=False,\n",
    "    device=device\n",
    ")\n",
    "\n",
    "vgg_history = vgg_trainer.train(\n",
    "    num_epochs=config.NUM_EPOCHS,\n",
    "    save_path=os.path.join(MODEL_PATH, 'vgg16_model.pth')\n",
    ")\n",
    "\n",
    "print(\"\\nVGG16 backbone training complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80cf858a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 60)\n",
    "print(\"TRAINING IMPROVED MODEL (U-NET + ATTENTION + VGG16)\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "improved_model = ImprovedColorizationModel(\n",
    "    use_attention=True,\n",
    "    use_global_features=True,\n",
    "    backbone='vgg16',\n",
    "    pretrained=True,\n",
    "    freeze_backbone=False\n",
    ")\n",
    "improved_model = improved_model.to(device)\n",
    "\n",
    "print(f\"Model parameters: {sum(p.numel() for p in improved_model.parameters()):,}\")\n",
    "print(f\"Trainable parameters: {sum(p.numel() for p in improved_model.parameters() if p.requires_grad):,}\")\n",
    "\n",
    "improved_trainer = Trainer(\n",
    "    model=improved_model,\n",
    "    train_loader=train_loader,\n",
    "    val_loader=val_loader,\n",
    "    config=config,\n",
    "    use_perceptual=True,\n",
    "    use_gan=False,\n",
    "    device=device\n",
    ")\n",
    "\n",
    "improved_history = improved_trainer.train(\n",
    "    num_epochs=config.NUM_EPOCHS,\n",
    "    save_path=os.path.join(MODEL_PATH, 'improved_model.pth')\n",
    ")\n",
    "\n",
    "print(\"\\nImproved model training complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de00afa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 60)\n",
    "print(\"TRAINING MODEL WITH PATCHGAN DISCRIMINATOR (BONUS)\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "gan_model = ImprovedColorizationModel(\n",
    "    use_attention=True,\n",
    "    use_global_features=True,\n",
    "    backbone='vgg16',\n",
    "    pretrained=True,\n",
    "    freeze_backbone=False\n",
    ")\n",
    "gan_model = gan_model.to(device)\n",
    "\n",
    "print(f\"Generator parameters: {sum(p.numel() for p in gan_model.parameters()):,}\")\n",
    "\n",
    "gan_trainer = Trainer(\n",
    "    model=gan_model,\n",
    "    train_loader=train_loader,\n",
    "    val_loader=val_loader,\n",
    "    config=config,\n",
    "    use_perceptual=True,\n",
    "    use_gan=True,\n",
    "    device=device\n",
    ")\n",
    "\n",
    "print(f\"Discriminator parameters: {sum(p.numel() for p in gan_trainer.discriminator.parameters()):,}\")\n",
    "\n",
    "gan_history = gan_trainer.train(\n",
    "    num_epochs=config.NUM_EPOCHS,\n",
    "    save_path=os.path.join(MODEL_PATH, 'gan_model.pth')\n",
    ")\n",
    "\n",
    "print(\"\\nGAN training complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee1dad5b",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Step 15: Visualization and Analysis\n",
    "\n",
    "Now we visualize:\n",
    "1. Training curves (loss, MSE, PSNR, SSIM over epochs)\n",
    "2. Sample colorization results from different models\n",
    "3. Comparison between different architectures and loss functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2156561e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_training_curves(histories, model_names, save_path=None):\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "    \n",
    "    colors = ['blue', 'orange', 'green', 'red', 'purple']\n",
    "    \n",
    "    ax = axes[0, 0]\n",
    "    for i, (history, name) in enumerate(zip(histories, model_names)):\n",
    "        if 'train_loss' in history and len(history['train_loss']) > 0:\n",
    "            ax.plot(history['train_loss'], color=colors[i], label=f'{name} (train)', linestyle='-')\n",
    "        if 'val_loss' in history and len(history['val_loss']) > 0:\n",
    "            ax.plot(history['val_loss'], color=colors[i], label=f'{name} (val)', linestyle='--')\n",
    "    ax.set_xlabel('Epoch')\n",
    "    ax.set_ylabel('Loss')\n",
    "    ax.set_title('Training and Validation Loss')\n",
    "    ax.legend(loc='upper right')\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    \n",
    "    ax = axes[0, 1]\n",
    "    for i, (history, name) in enumerate(zip(histories, model_names)):\n",
    "        if 'val_mse' in history and len(history['val_mse']) > 0:\n",
    "            ax.plot(history['val_mse'], color=colors[i], label=name)\n",
    "    ax.set_xlabel('Epoch')\n",
    "    ax.set_ylabel('MSE')\n",
    "    ax.set_title('Validation MSE')\n",
    "    ax.legend(loc='upper right')\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    \n",
    "    ax = axes[1, 0]\n",
    "    for i, (history, name) in enumerate(zip(histories, model_names)):\n",
    "        if 'val_psnr' in history and len(history['val_psnr']) > 0:\n",
    "            ax.plot(history['val_psnr'], color=colors[i], label=name)\n",
    "    ax.set_xlabel('Epoch')\n",
    "    ax.set_ylabel('PSNR (dB)')\n",
    "    ax.set_title('Validation PSNR')\n",
    "    ax.legend(loc='lower right')\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    \n",
    "    ax = axes[1, 1]\n",
    "    for i, (history, name) in enumerate(zip(histories, model_names)):\n",
    "        if 'val_ssim' in history and len(history['val_ssim']) > 0:\n",
    "            ax.plot(history['val_ssim'], color=colors[i], label=name)\n",
    "    ax.set_xlabel('Epoch')\n",
    "    ax.set_ylabel('SSIM')\n",
    "    ax.set_title('Validation SSIM')\n",
    "    ax.legend(loc='lower right')\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    \n",
    "    if save_path:\n",
    "        plt.savefig(save_path, dpi=150, bbox_inches='tight')\n",
    "        print(f\"Saved training curves to {save_path}\")\n",
    "    \n",
    "    plt.show()\n",
    "\n",
    "all_histories = [baseline_history, vgg_history, improved_history, gan_history]\n",
    "model_names = ['Baseline', 'VGG16 Backbone', 'Improved (Attention)', 'With PatchGAN']\n",
    "\n",
    "plot_training_curves(\n",
    "    all_histories, \n",
    "    model_names,\n",
    "    save_path=os.path.join(RESULTS_PATH, 'training_curves.png')\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d17b700c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_colorization_results(models, model_names, dataloader, num_samples=4, save_path=None):\n",
    "    L_batch, ab_batch = next(iter(dataloader))\n",
    "    L_batch = L_batch[:num_samples].to(device)\n",
    "    ab_batch = ab_batch[:num_samples].to(device)\n",
    "    \n",
    "    num_cols = 2 + len(models)\n",
    "    \n",
    "    fig, axes = plt.subplots(num_samples, num_cols, figsize=(4*num_cols, 4*num_samples))\n",
    "    \n",
    "    for i in range(num_samples):\n",
    "        L = L_batch[i:i+1]\n",
    "        ab_gt = ab_batch[i:i+1]\n",
    "        \n",
    "        L_np = (L[0, 0].cpu().numpy() + 1) * 50\n",
    "        ab_gt_np = ab_gt[0].cpu().numpy() * 128\n",
    "        \n",
    "        lab_gt = np.stack([L_np, ab_gt_np[0], ab_gt_np[1]], axis=-1)\n",
    "        rgb_gt = np.clip(color.lab2rgb(lab_gt), 0, 1)\n",
    "        \n",
    "        axes[i, 0].imshow(L_np, cmap='gray')\n",
    "        axes[i, 0].set_title('Grayscale Input' if i == 0 else '')\n",
    "        axes[i, 0].axis('off')\n",
    "        \n",
    "        axes[i, 1].imshow(rgb_gt)\n",
    "        axes[i, 1].set_title('Ground Truth' if i == 0 else '')\n",
    "        axes[i, 1].axis('off')\n",
    "        \n",
    "        for j, (model, name) in enumerate(zip(models, model_names)):\n",
    "            model.eval()\n",
    "            with torch.no_grad():\n",
    "                pred_ab = model(L)\n",
    "            \n",
    "            pred_ab_np = pred_ab[0].cpu().numpy() * 128\n",
    "            \n",
    "            lab_pred = np.stack([L_np, pred_ab_np[0], pred_ab_np[1]], axis=-1)\n",
    "            rgb_pred = np.clip(color.lab2rgb(lab_pred), 0, 1)\n",
    "            \n",
    "            axes[i, j+2].imshow(rgb_pred)\n",
    "            axes[i, j+2].set_title(name if i == 0 else '')\n",
    "            axes[i, j+2].axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    \n",
    "    if save_path:\n",
    "        plt.savefig(save_path, dpi=150, bbox_inches='tight')\n",
    "        print(f\"Saved colorization results to {save_path}\")\n",
    "    \n",
    "    plt.show()\n",
    "\n",
    "all_models = [baseline_model, vgg_model, improved_model, gan_model]\n",
    "model_names = ['Baseline', 'VGG16', 'Improved', 'PatchGAN']\n",
    "\n",
    "visualize_colorization_results(\n",
    "    all_models,\n",
    "    model_names,\n",
    "    val_loader,\n",
    "    num_samples=4,\n",
    "    save_path=os.path.join(RESULTS_PATH, 'colorization_results.png')\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28730ab2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, dataloader, device='cuda'):\n",
    "    model.eval()\n",
    "    \n",
    "    all_mse, all_psnr, all_ssim = [], [], []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for L, ab in tqdm(dataloader, desc='Evaluating'):\n",
    "            L = L.to(device)\n",
    "            ab = ab.to(device)\n",
    "            \n",
    "            pred_ab = model(L)\n",
    "            \n",
    "            metrics = EvaluationMetrics.evaluate_batch(pred_ab, ab, L)\n",
    "            all_mse.append(metrics['mse'])\n",
    "            all_psnr.append(metrics['psnr'])\n",
    "            all_ssim.append(metrics['ssim'])\n",
    "    \n",
    "    return {\n",
    "        'mse': np.mean(all_mse),\n",
    "        'psnr': np.mean(all_psnr),\n",
    "        'ssim': np.mean(all_ssim)\n",
    "    }\n",
    "\n",
    "def create_comparison_table(models, model_names, dataloader):\n",
    "    results = []\n",
    "    \n",
    "    for model, name in zip(models, model_names):\n",
    "        print(f\"Evaluating {name}...\")\n",
    "        metrics = evaluate_model(model, dataloader, device)\n",
    "        results.append({\n",
    "            'Model': name,\n",
    "            'MSE': metrics['mse'],\n",
    "            'PSNR (dB)': metrics['psnr'],\n",
    "            'SSIM': metrics['ssim']\n",
    "        })\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 70)\n",
    "    print(\"MODEL COMPARISON TABLE\")\n",
    "    print(\"=\" * 70)\n",
    "    print(f\"{'Model':<25} {'MSE':<15} {'PSNR (dB)':<15} {'SSIM':<15}\")\n",
    "    print(\"-\" * 70)\n",
    "    \n",
    "    for r in results:\n",
    "        print(f\"{r['Model']:<25} {r['MSE']:<15.6f} {r['PSNR (dB)']:<15.2f} {r['SSIM']:<15.4f}\")\n",
    "    \n",
    "    print(\"=\" * 70)\n",
    "    \n",
    "    return results\n",
    "\n",
    "all_models = [baseline_model, vgg_model, improved_model, gan_model]\n",
    "model_names = ['Baseline', 'VGG16 Backbone', 'Improved (Attention)', 'With PatchGAN']\n",
    "\n",
    "comparison_results = create_comparison_table(all_models, model_names, val_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67c3f68b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_metrics_comparison(results, save_path=None):\n",
    "    models = [r['Model'] for r in results]\n",
    "    mse_values = [r['MSE'] for r in results]\n",
    "    psnr_values = [r['PSNR (dB)'] for r in results]\n",
    "    ssim_values = [r['SSIM'] for r in results]\n",
    "    \n",
    "    fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
    "    colors = ['#3498db', '#e74c3c', '#2ecc71', '#9b59b6']\n",
    "    \n",
    "    ax = axes[0]\n",
    "    bars = ax.bar(models, mse_values, color=colors[:len(models)])\n",
    "    ax.set_ylabel('MSE')\n",
    "    ax.set_title('Mean Squared Error (Lower is Better)')\n",
    "    ax.tick_params(axis='x', rotation=45)\n",
    "    for bar, val in zip(bars, mse_values):\n",
    "        ax.text(bar.get_x() + bar.get_width()/2, bar.get_height(), \n",
    "               f'{val:.4f}', ha='center', va='bottom', fontsize=9)\n",
    "    \n",
    "    ax = axes[1]\n",
    "    bars = ax.bar(models, psnr_values, color=colors[:len(models)])\n",
    "    ax.set_ylabel('PSNR (dB)')\n",
    "    ax.set_title('Peak Signal-to-Noise Ratio (Higher is Better)')\n",
    "    ax.tick_params(axis='x', rotation=45)\n",
    "    for bar, val in zip(bars, psnr_values):\n",
    "        ax.text(bar.get_x() + bar.get_width()/2, bar.get_height(), \n",
    "               f'{val:.2f}', ha='center', va='bottom', fontsize=9)\n",
    "    \n",
    "    ax = axes[2]\n",
    "    bars = ax.bar(models, ssim_values, color=colors[:len(models)])\n",
    "    ax.set_ylabel('SSIM')\n",
    "    ax.set_title('Structural Similarity Index (Higher is Better)')\n",
    "    ax.tick_params(axis='x', rotation=45)\n",
    "    for bar, val in zip(bars, ssim_values):\n",
    "        ax.text(bar.get_x() + bar.get_width()/2, bar.get_height(), \n",
    "               f'{val:.4f}', ha='center', va='bottom', fontsize=9)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    \n",
    "    if save_path:\n",
    "        plt.savefig(save_path, dpi=150, bbox_inches='tight')\n",
    "        print(f\"Saved comparison charts to {save_path}\")\n",
    "    \n",
    "    plt.show()\n",
    "\n",
    "plot_metrics_comparison(\n",
    "    comparison_results,\n",
    "    save_path=os.path.join(config.RESULTS_PATH, 'metrics_comparison.png')\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a62513f7",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Step 16: Loss Function Comparison\n",
    "\n",
    "Here we compare the effects of different loss functions on colorization quality:\n",
    "1. **L1 Loss only**: Encourages pixel-wise accuracy\n",
    "2. **Perceptual Loss only**: Encourages semantic similarity\n",
    "3. **Combined (L1 + Perceptual)**: Best of both worlds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e6f964a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_loss_functions(train_loader, val_loader, num_epochs=3):\n",
    "    results = {}\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(\"Training with L1 Loss only...\")\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    model_l1 = ColorizationModel().to(device)\n",
    "    trainer_l1 = Trainer(\n",
    "        model=model_l1,\n",
    "        train_loader=train_loader,\n",
    "        val_loader=val_loader,\n",
    "        config=config,\n",
    "        use_perceptual=False,\n",
    "        use_gan=False,\n",
    "        device=device\n",
    "    )\n",
    "    history_l1 = trainer_l1.train(num_epochs)\n",
    "    results['L1 Only'] = {\n",
    "        'model': model_l1,\n",
    "        'history': history_l1,\n",
    "        'metrics': evaluate_model(model_l1, val_loader, device)\n",
    "    }\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(\"Training with Perceptual Loss + L1...\")\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    model_perceptual = ColorizationModel().to(device)\n",
    "    trainer_perceptual = Trainer(\n",
    "        model=model_perceptual,\n",
    "        train_loader=train_loader,\n",
    "        val_loader=val_loader,\n",
    "        config=config,\n",
    "        use_perceptual=True,\n",
    "        use_gan=False,\n",
    "        device=device\n",
    "    )\n",
    "    history_perceptual = trainer_perceptual.train(num_epochs)\n",
    "    results['L1 + Perceptual'] = {\n",
    "        'model': model_perceptual,\n",
    "        'history': history_perceptual,\n",
    "        'metrics': evaluate_model(model_perceptual, val_loader, device)\n",
    "    }\n",
    "    \n",
    "    return results\n",
    "\n",
    "print(\"Comparing different loss functions...\")\n",
    "loss_comparison = compare_loss_functions(train_loader, val_loader, num_epochs=50)\n",
    "\n",
    "print(\"\\nLoss function comparison complete!\")\n",
    "print(\"\\nDiscussion:\")\n",
    "print(\"-\" * 50)\n",
    "print(\"\"\"\n",
    "L1 LOSS ONLY:\n",
    "- Produces smooth, somewhat blurry colorizations\n",
    "- Good at preserving overall structure\n",
    "- May produce desaturated colors\n",
    "- Fast and stable training\n",
    "\n",
    "PERCEPTUAL LOSS:\n",
    "- Encourages more realistic textures\n",
    "- Better semantic consistency\n",
    "- Can produce more vibrant colors\n",
    "- Higher computational cost\n",
    "\n",
    "COMBINED (L1 + PERCEPTUAL):\n",
    "- Best of both approaches\n",
    "- Balances accuracy and perceptual quality\n",
    "- Most commonly used in practice\n",
    "- Requires careful weight balancing\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e51a5628",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Conclusions and Discussion\n",
    "\n",
    "### Summary of Results\n",
    "\n",
    "In this assignment, we implemented and compared several approaches for image colorization:\n",
    "\n",
    "1. **Baseline Encoder-Decoder**: A simple architecture that learns to map grayscale L channel to color a*b* channels.\n",
    "\n",
    "2. **Pretrained Backbone (VGG16/ResNet50)**: Using pretrained networks as global feature extractors improves semantic understanding and leads to more contextually appropriate colors.\n",
    "\n",
    "3. **Improved Architecture (U-Net + Attention)**: Skip connections preserve spatial details, while attention mechanisms help focus on important features.\n",
    "\n",
    "4. **PatchGAN Discriminator**: Adding adversarial training produces sharper, more locally consistent colors.\n",
    "\n",
    "### Key Findings\n",
    "\n",
    "- **Skip connections** significantly improve detail preservation in colorized images\n",
    "- **Pretrained backbones** provide better semantic understanding for choosing appropriate colors\n",
    "- **Perceptual loss** encourages more realistic textures compared to L1 alone\n",
    "- **PatchGAN** produces sharper results but requires careful training balance\n",
    "\n",
    "### Recommendations for Best Results\n",
    "\n",
    "1. Use a U-Net style architecture with skip connections\n",
    "2. Incorporate a pretrained VGG16 or ResNet50 as global feature extractor\n",
    "3. Combine L1 loss with perceptual loss (weight ratio ~10:1)\n",
    "4. Train for at least 50-100 epochs with a large, diverse dataset\n",
    "5. Consider PatchGAN for applications requiring sharp, realistic colors\n",
    "\n",
    "### Limitations and Future Work\n",
    "\n",
    "- The model may struggle with uncommon objects or scenes not well-represented in training data\n",
    "- Colorization of historical photos may require domain-specific training\n",
    "- User-guided colorization (hint-based) could improve accuracy\n",
    "- Video colorization would require temporal consistency constraints"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
